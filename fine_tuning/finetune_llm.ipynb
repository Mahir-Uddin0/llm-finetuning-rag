{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-23T21:06:58.765999Z",
     "iopub.status.busy": "2025-10-23T21:06:58.765362Z",
     "iopub.status.idle": "2025-10-23T21:08:30.031300Z",
     "shell.execute_reply": "2025-10-23T21:08:30.030188Z",
     "shell.execute_reply.started": "2025-10-23T21:06:58.765971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade pip\n",
    "!pip install -q \"transformers>=4.30.0\" \"datasets\" \"accelerate>=0.21.0\" \"peft>=0.4.0\" \"bitsandbytes>=0.39.0\" \"sentencepiece\" \"safetensors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T13:49:52.199183Z",
     "iopub.status.busy": "2025-10-26T13:49:52.198837Z",
     "iopub.status.idle": "2025-10-26T13:51:05.769998Z",
     "shell.execute_reply": "2025-10-26T13:51:05.768926Z",
     "shell.execute_reply.started": "2025-10-26T13:49:52.199155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
      "Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 1.0.0rc2\n",
      "    Uninstalling huggingface-hub-1.0.0rc2:\n",
      "      Successfully uninstalled huggingface-hub-1.0.0rc2\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.36.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T13:51:05.771438Z",
     "iopub.status.busy": "2025-10-26T13:51:05.771173Z",
     "iopub.status.idle": "2025-10-26T13:52:17.574885Z",
     "shell.execute_reply": "2025-10-26T13:52:17.574243Z",
     "shell.execute_reply.started": "2025-10-26T13:51:05.771405Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "2025-10-26 13:51:22.978318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761486683.193944      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761486683.250756      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5f837c1a254a9d995034e77d76e98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"/kaggle/input/llama-3.2-3b-instruct/pytorch/default/1\"   # ← change this to your model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",      # loads layers automatically on available GPUs/CPU\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T14:17:02.908372Z",
     "iopub.status.busy": "2025-10-26T14:17:02.908094Z",
     "iopub.status.idle": "2025-10-26T14:17:02.934439Z",
     "shell.execute_reply": "2025-10-26T14:17:02.933498Z",
     "shell.execute_reply.started": "2025-10-26T14:17:02.908350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Linear Layers:\n",
      "========================================\n",
      "model.layers.0.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.0.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.0.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.0.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.0.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\n",
      "model.layers.0.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\n",
      "model.layers.0.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\n",
      "model.layers.1.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.1.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.1.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.1.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.1.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\n",
      "model.layers.1.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\n",
      "model.layers.1.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\n",
      "model.layers.2.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.2.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.2.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.2.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.2.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\n",
      "model.layers.2.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\n",
      "model.layers.2.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\n",
      "model.layers.3.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.3.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.3.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.3.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.3.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\n",
      "model.layers.3.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\n",
      "model.layers.3.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\n",
      "model.layers.4.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.4.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.4.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.4.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.4.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\n",
      "model.layers.4.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\n",
      "model.layers.4.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\n",
      "model.layers.5.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.5.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.5.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.5.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.5.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\n",
      "model.layers.5.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\n",
      "model.layers.5.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\n",
      "model.layers.6.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.6.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.6.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.6.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.6.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\n",
      "model.layers.6.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\n",
      "model.layers.6.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\n",
      "model.layers.7.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.7.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.7.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.7.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.7.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\n",
      "model.layers.7.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\n",
      "model.layers.7.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\n",
      "model.layers.8.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.8.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.8.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.8.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.8.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\n",
      "model.layers.8.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\n",
      "model.layers.8.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\n",
      "model.layers.9.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.9.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.9.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\n",
      "model.layers.9.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\n",
      "model.layers.9.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\n",
      "model.layers.9.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\n",
      "model.layers.9.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\n",
      "model.layers.10.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.10.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.10.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.10.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.10.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\n",
      "model.layers.10.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\n",
      "model.layers.10.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\n",
      "model.layers.11.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.11.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.11.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.11.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.11.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\n",
      "model.layers.11.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\n",
      "model.layers.11.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\n",
      "model.layers.12.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.12.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.12.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.12.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.12.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\n",
      "model.layers.12.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\n",
      "model.layers.12.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\n",
      "model.layers.13.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.13.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.13.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.13.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.13.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\n",
      "model.layers.13.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\n",
      "model.layers.13.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\n",
      "model.layers.14.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.14.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.14.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.14.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.14.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\n",
      "model.layers.14.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\n",
      "model.layers.14.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\n",
      "model.layers.15.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.15.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.15.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.15.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.15.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\n",
      "model.layers.15.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\n",
      "model.layers.15.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\n",
      "model.layers.16.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.16.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.16.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.16.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.16.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\n",
      "model.layers.16.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\n",
      "model.layers.16.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\n",
      "model.layers.17.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.17.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.17.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.17.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.17.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\n",
      "model.layers.17.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\n",
      "model.layers.17.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\n",
      "model.layers.18.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.18.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.18.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.18.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.18.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\n",
      "model.layers.18.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\n",
      "model.layers.18.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\n",
      "model.layers.19.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.19.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.19.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.19.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.19.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\n",
      "model.layers.19.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\n",
      "model.layers.19.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\n",
      "model.layers.20.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.20.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.20.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.20.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.20.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\n",
      "model.layers.20.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\n",
      "model.layers.20.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\n",
      "model.layers.21.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.21.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.21.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.21.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.21.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\n",
      "model.layers.21.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\n",
      "model.layers.21.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\n",
      "model.layers.22.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.22.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.22.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.22.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.22.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\n",
      "model.layers.22.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\n",
      "model.layers.22.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\n",
      "model.layers.23.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.23.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.23.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.23.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.23.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\n",
      "model.layers.23.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\n",
      "model.layers.23.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\n",
      "model.layers.24.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.24.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.24.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.24.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.24.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\n",
      "model.layers.24.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\n",
      "model.layers.24.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\n",
      "model.layers.25.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.25.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.25.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.25.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.25.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\n",
      "model.layers.25.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\n",
      "model.layers.25.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\n",
      "model.layers.26.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.26.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.26.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.26.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.26.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\n",
      "model.layers.26.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\n",
      "model.layers.26.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\n",
      "model.layers.27.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.27.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.27.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\n",
      "model.layers.27.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\n",
      "model.layers.27.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\n",
      "model.layers.27.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\n",
      "model.layers.27.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\n",
      "lm_head                                                                 128256 x 3072   = 394,002,432 params\n",
      "\n",
      "Total trainable parameters: 3212749824\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "summary(model, depth=9, col_names=[\"kernel_size\", \"num_params\", \"trainable\"])\n",
    "\n",
    "print(\"\\nDetailed Linear Layers:\\n\" + \"=\"*40)\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        d, k = module.weight.shape\n",
    "        print(f\"{name:<70}  {d:>5} x {k:<5}  = {d*k:,} params\")\n",
    "        \n",
    "print(\"\\nTotal trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T14:07:03.278685Z",
     "iopub.status.busy": "2025-10-26T14:07:03.278371Z",
     "iopub.status.idle": "2025-10-26T14:07:03.320326Z",
     "shell.execute_reply": "2025-10-26T14:07:03.319701Z",
     "shell.execute_reply.started": "2025-10-26T14:07:03.278661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRANSFORMER BLOCK STRUCTURE ===\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "    (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ")\n",
      "+--------------------------+----------------+------------+\n",
      "|        Layer Name        |      Type      | Parameters |\n",
      "+--------------------------+----------------+------------+\n",
      "|        self_attn         | LlamaAttention | 25,165,824 |\n",
      "|           q_proj         |     Linear     | 9,437,184  |\n",
      "|           k_proj         |     Linear     | 3,145,728  |\n",
      "|           v_proj         |     Linear     | 3,145,728  |\n",
      "|           o_proj         |     Linear     | 9,437,184  |\n",
      "|           mlp            |    LlamaMLP    | 75,497,472 |\n",
      "|         gate_proj        |     Linear     | 25,165,824 |\n",
      "|          up_proj         |     Linear     | 25,165,824 |\n",
      "|         down_proj        |     Linear     | 25,165,824 |\n",
      "|           act_fn         |      SiLU      |     0      |\n",
      "|     input_layernorm      |  LlamaRMSNorm  |   3,072    |\n",
      "| post_attention_layernorm |  LlamaRMSNorm  |   3,072    |\n",
      "+--------------------------+----------------+------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Get one transformer block (acts like one encoder/decoder layer)\n",
    "block = model.model.layers[0]\n",
    "\n",
    "print(\"=== TRANSFORMER BLOCK STRUCTURE ===\")\n",
    "print(block)\n",
    "\n",
    "# Function to recursively list modules and parameter counts\n",
    "def describe_module(module, prefix=\"\"):\n",
    "    rows = []\n",
    "    total_params = 0\n",
    "    for name, child in module.named_children():\n",
    "        params = sum(p.numel() for p in child.parameters())\n",
    "        total_params += params\n",
    "        rows.append([prefix + name, child.__class__.__name__, f\"{params:,}\"])\n",
    "        rows += describe_module(child, prefix + \"  \")\n",
    "    return rows\n",
    "\n",
    "rows = describe_module(block)\n",
    "\n",
    "# Pretty print the architecture\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Layer Name\", \"Type\", \"Parameters\"]\n",
    "for r in rows:\n",
    "    table.add_row(r)\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T14:07:51.626970Z",
     "iopub.status.busy": "2025-10-26T14:07:51.626356Z",
     "iopub.status.idle": "2025-10-26T14:07:51.631066Z",
     "shell.execute_reply": "2025-10-26T14:07:51.630162Z",
     "shell.execute_reply.started": "2025-10-26T14:07:51.626945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of decoder layers: 28\n"
     ]
    }
   ],
   "source": [
    "num_layers = len(model.model.layers)\n",
    "print(f\"Number of decoder layers: {num_layers}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8559109,
     "sourceId": 13481404,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 127982,
     "modelInstanceId": 103765,
     "sourceId": 123303,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
