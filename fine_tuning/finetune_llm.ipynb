{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13481404,"sourceType":"datasetVersion","datasetId":8559109},{"sourceId":13511744,"sourceType":"datasetVersion","datasetId":8578867},{"sourceId":123303,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":103765,"modelId":127982},{"sourceId":621049,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":467068,"modelId":482893}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q --upgrade pip\n!pip install -q \"transformers>=4.30.0\" \"datasets\" \"accelerate>=0.21.0\" \"peft>=0.4.0\" \"bitsandbytes>=0.39.0\" \"sentencepiece\" \"safetensors\" \"torchinfo\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-27T00:10:28.317493Z","iopub.execute_input":"2025-10-27T00:10:28.317764Z","iopub.status.idle":"2025-10-27T00:12:06.267080Z","shell.execute_reply.started":"2025-10-27T00:10:28.317736Z","shell.execute_reply":"2025-10-27T00:12:06.266241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers accelerate torchinfo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T00:12:06.269602Z","iopub.execute_input":"2025-10-27T00:12:06.269874Z","iopub.status.idle":"2025-10-27T00:12:08.314477Z","shell.execute_reply.started":"2025-10-27T00:12:06.269852Z","shell.execute_reply":"2025-10-27T00:12:08.313788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = \"/kaggle/input/llama-3.2-3b-instruct/pytorch/default/1\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",      # loads layers automatically on available GPUs/CPU\n    torch_dtype=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T00:12:08.315331Z","iopub.execute_input":"2025-10-27T00:12:08.315571Z","iopub.status.idle":"2025-10-27T00:17:05.191254Z","shell.execute_reply.started":"2025-10-27T00:12:08.315541Z","shell.execute_reply":"2025-10-27T00:17:05.190617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image, display\n\nimage_path = \"/kaggle/input/llama-architecture/llama32_architecture.jpg\"\ndisplay(Image(filename=image_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T00:17:05.192201Z","iopub.execute_input":"2025-10-27T00:17:05.193436Z","iopub.status.idle":"2025-10-27T00:17:05.222017Z","shell.execute_reply.started":"2025-10-27T00:17:05.193412Z","shell.execute_reply":"2025-10-27T00:17:05.221317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom prettytable import PrettyTable\n\n# === Load Model ===\nMODEL_NAME = \"/kaggle/input/llama-3.2-3b-instruct/pytorch/default/1\"\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\", torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# === Get one decoder layer ===\nlayer = model.model.layers[0]\n\n# === Create PrettyTable ===\ntable = PrettyTable()\ntable.field_names = [\"Layer Name\", \"Type\", \"Input Dim\", \"Output Dim\", \"Weight Shape\", \"Parameters\"]\n\n# Helper function for adding Linear layers\ndef add_linear(name, linear_module):\n    in_dim = linear_module.in_features\n    out_dim = linear_module.out_features\n    shape = tuple(linear_module.weight.shape)\n    params = linear_module.weight.numel()\n    table.add_row([name, \"Linear\", in_dim, out_dim, shape, f\"{params:,}\"])\n    return params\n\n# === SELF-ATTENTION ===\nattn = layer.self_attn\nsa_total = 0\nsa_total += add_linear(\"q_proj\", attn.q_proj)\nsa_total += add_linear(\"k_proj\", attn.k_proj)\nsa_total += add_linear(\"v_proj\", attn.v_proj)\nsa_total += add_linear(\"o_proj\", attn.o_proj)\ntable.add_row([\"‚Üí self_attn (total)\", \"LlamaAttention\", \"-\", \"-\", \"-\", f\"{sa_total:,}\"])\n\n# === MLP ===\nmlp = layer.mlp\nmlp_total = 0\nmlp_total += add_linear(\"gate_proj\", mlp.gate_proj)\nmlp_total += add_linear(\"up_proj\", mlp.up_proj)\nmlp_total += add_linear(\"down_proj\", mlp.down_proj)\ntable.add_row([\"‚Üí mlp (total)\", \"LlamaMLP\", \"-\", \"-\", \"-\", f\"{mlp_total:,}\"])\n\n# === LayerNorms ===\nfor name, norm in [(\"input_layernorm\", layer.input_layernorm),\n                   (\"post_attention_layernorm\", layer.post_attention_layernorm)]:\n    params = sum(p.numel() for p in norm.parameters())\n    table.add_row([name, \"LlamaRMSNorm\", \"-\", \"-\", \"-\", f\"{params:,}\"])\n\n# === Total for 1 Decoder Layer ===\ndecoder_total = sa_total + mlp_total\nfor norm in [layer.input_layernorm, layer.post_attention_layernorm]:\n    decoder_total += sum(p.numel() for p in norm.parameters())\ntable.add_row([\"TOTAL (1 Decoder Layer)\", \"LlamaDecoderLayer\", \"-\", \"-\", \"-\", f\"{decoder_total:,}\"])\n\nnum_layers = len(model.model.layers)\ntable.add_row([f\"TOTAL ({num_layers} Decoder Layer)\", \"LlamaDecoderLayer\", \"-\", \"-\", \"-\", f\"{decoder_total * num_layers:,}\"]) # Parameter for all decoder layers\n\n# # lm head\n# lm_head = model.get_output_embeddings()\n# params = lm_head.weight.numel()\n# table.add_row([\"lm_head\", \"Linear\", in_dim, out_dim, shape, f\"{params:,}\"])\n\n# # Total parameters\n# table.add_row([\"Total Parameters\", \"Linear\", in_dim, out_dim, shape, f\"{decoder_total * num_layers + params:,}\"])\n\n# === PRINT TABLE ===\nprint(table)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T00:17:05.222704Z","iopub.execute_input":"2025-10-27T00:17:05.222905Z","iopub.status.idle":"2025-10-27T00:17:08.466795Z","shell.execute_reply.started":"2025-10-27T00:17:05.222890Z","shell.execute_reply":"2025-10-27T00:17:08.466147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Number of decoder layers: {num_layers}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T00:17:08.467488Z","iopub.execute_input":"2025-10-27T00:17:08.467777Z","iopub.status.idle":"2025-10-27T00:17:08.471796Z","shell.execute_reply.started":"2025-10-27T00:17:08.467759Z","shell.execute_reply":"2025-10-27T00:17:08.471003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport json\nimport glob\nfrom pathlib import Path\nimport torch\n\nprint(\"Python:\", sys.version.splitlines()[0])\nprint(\"Torch:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"CUDA device:\", torch.cuda.get_device_name(0))\nelse:\n    print(\"Warning: CUDA not available.\")\n\nimport transformers, datasets, peft, bitsandbytes, accelerate\nprint(\"transformers:\", transformers.__version__)\nprint(\"datasets:\", datasets.__version__)\nprint(\"peft:\", peft.__version__)\nimport bitsandbytes as bnb\nprint(\"bitsandbytes:\", bnb.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T00:17:08.473771Z","iopub.execute_input":"2025-10-27T00:17:08.473945Z","iopub.status.idle":"2025-10-27T00:17:16.049594Z","shell.execute_reply.started":"2025-10-27T00:17:08.473933Z","shell.execute_reply":"2025-10-27T00:17:16.048980Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/finance-finetuning-dataset-combined\"\nBASE_MODEL_PATH = \"/kaggle/input/llama-3.2-3b-instruct/pytorch/default/1\"\nOUTPUT_DIR = \"/kaggle/working/qlora_lora_finance\"  # where adapters/checkpoints will be saved\n\n# Training hyperparameters\nMICRO_BATCH_SIZE = 2       \nGRAD_ACCUM_STEPS = 16          \nEPOCHS = 3\nLEARNING_RATE = 2e-3\nCUTOFF_LEN = 1024              # token sequence length (common for LLaMA-like models)\nLORA_R = 4\nLORA_ALPHA = 32\nLORA_DROPOUT = 0.05\nSEED = 42\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(\"DATA_PATH:\", DATA_PATH)\nprint(\"BASE_MODEL_PATH:\", BASE_MODEL_PATH)\nprint(\"OUTPUT_DIR:\", OUTPUT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:08:24.975299Z","iopub.execute_input":"2025-10-27T08:08:24.975818Z","iopub.status.idle":"2025-10-27T08:08:24.980939Z","shell.execute_reply.started":"2025-10-27T08:08:24.975793Z","shell.execute_reply":"2025-10-27T08:08:24.980278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(\"Loading tokenizer from:\", BASE_MODEL_PATH)\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, use_fast=False, trust_remote_code=True)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# BitsAndBytes 4-bit config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nprint(\"Loading base model (4-bit)\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_PATH,\n    device_map={\"\": torch.cuda.current_device()},\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n)\n\n# Freeze base model parameters\nfor param in model.parameters():\n    param.requires_grad = False\n\nprint(\"Model loaded. Parameetrs are freezed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:20:24.627035Z","iopub.execute_input":"2025-10-27T08:20:24.627342Z","iopub.status.idle":"2025-10-27T08:20:33.644995Z","shell.execute_reply.started":"2025-10-27T08:20:24.627320Z","shell.execute_reply":"2025-10-27T08:20:33.644212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n# Prepare the model for k-bit training (adjusts some layers / casts)\nmodel = prepare_model_for_kbit_training(model)\n\n# PEFT will ignore them. Typical attention projection names for LLaMA-like models:\ntarget_modules = [\n    \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n    \"gate_proj\", \"down_proj\", \"up_proj\",\n    \"wi\", \"wo\", \"wq\", \"wv\", \"wk\", \"wo\"\n]\n\nlora_config = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=target_modules,\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()  # show how many params will be trained\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:28:04.988073Z","iopub.execute_input":"2025-10-27T08:28:04.988821Z","iopub.status.idle":"2025-10-27T08:28:04.992502Z","shell.execute_reply.started":"2025-10-27T08:28:04.988799Z","shell.execute_reply":"2025-10-27T08:28:04.991709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for name, param in model.named_parameters():\n#     if \"lora\" in name.lower():\n#         print(f\"{name:<100}  shape={tuple(param.shape)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T00:17:25.627224Z","iopub.execute_input":"2025-10-27T00:17:25.627581Z","iopub.status.idle":"2025-10-27T00:17:25.631249Z","shell.execute_reply.started":"2025-10-27T00:17:25.627557Z","shell.execute_reply":"2025-10-27T00:17:25.630402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset, Dataset, DatasetDict\n\ndef find_data_files(path):\n    p = Path(path)\n    if p.is_file():\n        return [str(p)]\n    # If it's a dir, look for .jsonl/.json/.csv\n    files = []\n    for ext in (\"*.jsonl\",\"*.json\",\"*.csv\"):\n        files.extend(sorted([str(x) for x in p.glob(ext)]))\n    return files\n\ndata_files = find_data_files(DATA_PATH)\nif len(data_files) == 0:\n    raise FileNotFoundError(f\"No dataset files found at {DATA_PATH}. Expect .jsonl/.json/.csv or a file path.\")\nprint(\"Found data files:\", data_files[:10])\n\n# The dataset has fields: instruction, input, output (answer)\nsample_file = data_files[0]\nif sample_file.endswith(\".csv\"):\n    raw = load_dataset(\"csv\", data_files=data_files)\nelse:\n    # try json (datasets handles jsonl too)\n    raw = load_dataset(\"json\", data_files=data_files)\n\n# If the dataset has splits, unify to train split\nif \"train\" in raw:\n    ds = raw[\"train\"]\nelse:\n    # raw could be a single split\n    split_name = list(raw.keys())[0]\n    ds = raw[split_name]\n\nprint(\"Dataset size:\", len(ds))\nprint(\"Some columns:\", ds.column_names)\nprint(\"Sample record:\", ds[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:20:33.925064Z","iopub.status.idle":"2025-10-27T08:20:33.925399Z","shell.execute_reply.started":"2025-10-27T08:20:33.925232Z","shell.execute_reply":"2025-10-27T08:20:33.925248Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\nfrom transformers import PreTrainedTokenizerBase\n\nPROMPT_TEMPLATE = (\n    \"### Instruction:\\n{instruction}\\n\\n\"\n    \"### Input:\\n{input}\\n\\n\"\n    \"### Response:\\n\"\n)\n\ndef make_prompt(example):\n    instruction = example.get(\"instruction\") or example.get(\"Instruction\") or example.get(\"prompt\") or \"\"\n    inp = example.get(\"input\") or example.get(\"Input\") or \"\"\n    # If dataset uses 'output' or 'answer' keys:\n    output = example.get(\"output\") or example.get(\"Output\") or example.get(\"answer\") or example.get(\"Answer\") or example.get(\"response\") or \"\"\n    prompt = PROMPT_TEMPLATE.format(instruction=instruction.strip(), input=inp.strip())\n    full = prompt + output.strip()\n    return prompt, full, output\n\ndef tokenize_function(example, tokenizer: PreTrainedTokenizerBase):\n    prompt, full, output = make_prompt(example)\n    # tokenize prompt and full separately to find token lengths\n    tokenized_full = tokenizer(full, truncation=True, max_length=CUTOFF_LEN, padding=False)\n    tokenized_prompt = tokenizer(prompt, truncation=True, max_length=CUTOFF_LEN, padding=False)\n    input_ids = tokenized_full[\"input_ids\"]\n    prompt_len = len(tokenized_prompt[\"input_ids\"])\n    labels = input_ids.copy()\n    # mask prompt tokens - so loss is computed only on the response portion\n    labels[:prompt_len] = [-100] * prompt_len\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": tokenized_full[\"attention_mask\"],\n        \"labels\": labels,\n    }\n\n# Quick tokenization test on a small sample\ntok_test = tokenize_function(ds[0], tokenizer)\nprint(\"tokenized sample lengths:\", {k: len(v) for k,v in tok_test.items()})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:20:34.279731Z","iopub.execute_input":"2025-10-27T08:20:34.280008Z","iopub.status.idle":"2025-10-27T08:20:34.296895Z","shell.execute_reply.started":"2025-10-27T08:20:34.279989Z","shell.execute_reply":"2025-10-27T08:20:34.296066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Shuffle + split\nds = ds.shuffle(seed=SEED)\n\n# small validation split: 2-5% (adjust as dataset size)\nval_size = max(1, int(0.02 * len(ds)))\ntrain_ds = ds.select(range(len(ds) - val_size))\nval_ds = ds.select(range(len(ds) - val_size, len(ds)))\n\nprint(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds))\n\n# Tokenize with batched mapping\ndef batched_tokenize(batch):\n    results = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n    for i in range(len(batch[next(iter(batch.keys()))])):  # iterate rows\n        ex = {k: batch[k][i] for k in batch.keys()}\n        tok = tokenize_function(ex, tokenizer)\n        results[\"input_ids\"].append(tok[\"input_ids\"])\n        results[\"attention_mask\"].append(tok[\"attention_mask\"])\n        results[\"labels\"].append(tok[\"labels\"])\n    return results\n\n# Use batched mapping\ntrain_tok = train_ds.map(batched_tokenize, batched=True, remove_columns=train_ds.column_names)\nval_tok = val_ds.map(batched_tokenize, batched=True, remove_columns=val_ds.column_names)\n\nprint(\"Tokenized train columns:\", train_tok.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:20:39.071873Z","iopub.execute_input":"2025-10-27T08:20:39.072146Z","iopub.status.idle":"2025-10-27T08:21:01.772514Z","shell.execute_reply.started":"2025-10-27T08:20:39.072128Z","shell.execute_reply":"2025-10-27T08:21:01.771760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Dict, List\nimport torch\nfrom transformers.tokenization_utils_base import PaddingStrategy\n\n@dataclass\nclass DataCollatorForCausalLM:\n    tokenizer: PreTrainedTokenizerBase\n    padding: bool = True\n    max_length: int = CUTOFF_LEN\n\n    def __call__(self, batch: List[Dict[str, List[int]]]) -> Dict[str, torch.Tensor]:\n        input_ids = [torch.tensor(b[\"input_ids\"], dtype=torch.long) for b in batch]\n        attention_mask = [torch.tensor(b[\"attention_mask\"], dtype=torch.long) for b in batch]\n        labels = [torch.tensor(b[\"labels\"], dtype=torch.long) for b in batch]\n        # pad using tokenizer.pad\n        pad_token_id = self.tokenizer.pad_token_id\n        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=pad_token_id)\n        attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n\ndata_collator = DataCollatorForCausalLM(tokenizer=tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:21:01.773544Z","iopub.execute_input":"2025-10-27T08:21:01.773800Z","iopub.status.idle":"2025-10-27T08:21:01.781174Z","shell.execute_reply.started":"2025-10-27T08:21:01.773783Z","shell.execute_reply":"2025-10-27T08:21:01.780317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\nimport inspect\nimport torch\nimport shutil\nfrom IPython.display import FileLink\nimport os\n\n# -------------------------------\n# 1Ô∏è‚É£ Prepare TrainingArguments\n# -------------------------------\ndesired_args = dict(\n    output_dir=OUTPUT_DIR,\n    per_device_train_batch_size=MICRO_BATCH_SIZE,\n    per_device_eval_batch_size=MICRO_BATCH_SIZE,\n    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n    num_train_epochs=EPOCHS,\n    learning_rate=LEARNING_RATE,\n    fp16=(True if torch.cuda.is_available() else False),\n    logging_steps=50,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"epoch\",       # <-- save after each epoch\n    save_total_limit=3,          # keep only last 3 epochs\n    load_best_model_at_end=False,\n    report_to=\"none\",\n    ddp_find_unused_parameters=(False if torch.cuda.is_available() else None),\n)\n\n# Filter only supported args\nsig = inspect.signature(TrainingArguments.__init__)\nsupported_params = set(sig.parameters.keys())\nsupported_params.discard('self')\nfiltered_args = {k: v for k, v in desired_args.items() if (k in supported_params and v is not None)}\n\ntraining_args = TrainingArguments(**filtered_args)\n\n# -------------------------------\n# 2Ô∏è‚É£ Create Trainer\n# -------------------------------\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_tok,\n    eval_dataset=val_tok,\n    data_collator=data_collator,\n)\n\nprint(\"Trainer created. Starting training...\")\n\n# -------------------------------\n# 3Ô∏è‚É£ Train and resume if checkpoint exists\n# -------------------------------\n# Resume automatically if OUTPUT_DIR has checkpoints\ndef get_last_checkpoint(output_dir):\n    checkpoints = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint\")]\n    if checkpoints:\n        # Return the latest checkpoint (sorted numerically)\n        return os.path.join(output_dir, sorted(checkpoints, key=lambda x: int(x.split('-')[-1]))[-1])\n    return None\nlast_ckpt = get_last_checkpoint(OUTPUT_DIR)\n\nif last_ckpt:\n    print(f\"Resuming training from checkpoint: {last_ckpt}\")\n    trainer.train(resume_from_checkpoint=last_ckpt)\nelse:\n    print(\"No checkpoint found. Starting training from scratch.\")\n    trainer.train()\n\n# -------------------------------\n# 4Ô∏è‚É£ Save final adapters and tokenizer\n# -------------------------------\nmodel.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(f\"Final adapters saved to: {OUTPUT_DIR}\")\n\n# -------------------------------\n# 5Ô∏è‚É£ Zip the output directory for download\n# -------------------------------\nzip_path = OUTPUT_DIR + \".zip\"\nshutil.make_archive(OUTPUT_DIR, 'zip', OUTPUT_DIR)\nprint(f\"Zipped output directory: {zip_path}\")\n\n# Provide a clickable download link\nFileLink(zip_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T00:17:47.828172Z","iopub.execute_input":"2025-10-27T00:17:47.828894Z","iopub.status.idle":"2025-10-27T06:51:45.053903Z","shell.execute_reply.started":"2025-10-27T00:17:47.828870Z","shell.execute_reply":"2025-10-27T06:51:45.053040Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport json\nimport os\n\nlog_path = os.path.join(\"/kaggle/working/qlora_lora_finance/checkpoint-616/trainer_state.json\")\n\nwith open(log_path, \"r\") as f:\n    logs = json.load(f)\n\ntrain_loss = [entry[\"loss\"] for entry in logs[\"log_history\"] if \"loss\" in entry]\nsteps = list(range(1, len(train_loss) + 1))\n\nplt.plot(steps, train_loss)\nplt.xlabel(\"Training Steps\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss vs Steps\")\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T07:06:39.004634Z","iopub.execute_input":"2025-10-27T07:06:39.005509Z","iopub.status.idle":"2025-10-27T07:06:39.183492Z","shell.execute_reply.started":"2025-10-27T07:06:39.005481Z","shell.execute_reply":"2025-10-27T07:06:39.182701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import AutoPeftModelForCausalLM\n\nmerged_model_dir = os.path.join(\"/kaggle/working/\", \"fine-tuned_model\")\nos.makedirs(merged_model_dir, exist_ok=True)\n\n# Load your PEFT model\npeft_model = AutoPeftModelForCausalLM.from_pretrained(final_model_dir, device_map=\"auto\")\n\n# Merge adapter + base weights\nmerged_model = peft_model.merge_and_unload()\n\n# Save the full fine-tuned model\nmerged_model.save_pretrained(merged_model_dir, safe_serialization=True)\ntokenizer.save_pretrained(merged_model_dir)\n\nprint(f\"‚úÖ Full merged model saved to: {merged_model_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T07:32:56.567366Z","iopub.execute_input":"2025-10-27T07:32:56.567699Z","iopub.status.idle":"2025-10-27T07:33:50.507313Z","shell.execute_reply.started":"2025-10-27T07:32:56.567677Z","shell.execute_reply":"2025-10-27T07:33:50.504626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_model_dir = os.path.join(\"/kaggle/working/\", \"LoRA_Weights\")\n\nos.makedirs(final_model_dir, exist_ok=True)\ntrainer.model.save_pretrained(final_model_dir)\ntokenizer.save_pretrained(final_model_dir)\n\nprint(f\"‚úÖ Fine-tuned model and tokenizer saved to: {final_model_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T07:17:29.510195Z","iopub.execute_input":"2025-10-27T07:17:29.510820Z","iopub.status.idle":"2025-10-27T07:17:29.852059Z","shell.execute_reply.started":"2025-10-27T07:17:29.510794Z","shell.execute_reply":"2025-10-27T07:17:29.851396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Zip the final model for download\n# -------------------------------\nzip_path = final_model_dir + \".zip\"\nshutil.make_archive(final_model_dir, 'zip', final_model_dir)\nprint(f\"üì¶ Zipped final model: {zip_path}\")\n\ndisplay(FileLink(zip_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T07:18:24.956248Z","iopub.execute_input":"2025-10-27T07:18:24.956560Z","iopub.status.idle":"2025-10-27T07:18:26.615597Z","shell.execute_reply.started":"2025-10-27T07:18:24.956537Z","shell.execute_reply":"2025-10-27T07:18:26.615008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import PeftModel\n\nprint(\"Loading base model and applying LoRA adapters for inference...\")\nbase = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_PATH,\n    device_map=\"auto\",\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n)\n\n# Load LoRA weights (this will attach them to base model)\nfrom peft import PeftModel\nlora_model = PeftModel.from_pretrained(base, OUTPUT_DIR, torch_dtype=torch.float16)\nlora_model.eval()\n\n# Example inference\ndef generate(prompt, max_new_tokens=256, temperature=0.2, top_p=0.95):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(lora_model.device)\n    with torch.no_grad():\n        out = lora_model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=temperature,\n            top_p=top_p,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n        )\n    return tokenizer.decode(out[0], skip_special_tokens=True)\n\n# Test with a small prompt:\nsample_prompt = PROMPT_TEMPLATE.format(instruction=\"Explain what a dividend yield is\", input=\"\")\nprint(generate(sample_prompt, max_new_tokens=120))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T06:51:45.059616Z","iopub.execute_input":"2025-10-27T06:51:45.059905Z","iopub.status.idle":"2025-10-27T06:52:09.190129Z","shell.execute_reply.started":"2025-10-27T06:51:45.059885Z","shell.execute_reply":"2025-10-27T06:52:09.189313Z"}},"outputs":[],"execution_count":null}]}