{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13481404,"sourceType":"datasetVersion","datasetId":8559109},{"sourceId":123303,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":103765,"modelId":127982}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q --upgrade pip\n!pip install -q \"transformers>=4.30.0\" \"datasets\" \"accelerate>=0.21.0\" \"peft>=0.4.0\" \"bitsandbytes>=0.39.0\" \"sentencepiece\" \"safetensors\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-23T21:06:58.765362Z","iopub.execute_input":"2025-10-23T21:06:58.765999Z","iopub.status.idle":"2025-10-23T21:08:30.031300Z","shell.execute_reply.started":"2025-10-23T21:06:58.765971Z","shell.execute_reply":"2025-10-23T21:08:30.030188Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install transformers accelerate torchinfo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T13:49:52.198837Z","iopub.execute_input":"2025-10-26T13:49:52.199183Z","iopub.status.idle":"2025-10-26T13:51:05.769998Z","shell.execute_reply.started":"2025-10-26T13:49:52.199155Z","shell.execute_reply":"2025-10-26T13:51:05.768926Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\nRequirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\nCollecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.36.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_NAME = \"/kaggle/input/llama-3.2-3b-instruct/pytorch/default/1\"   # ← change this to your model\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",      # loads layers automatically on available GPUs/CPU\n    torch_dtype=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T13:51:05.771173Z","iopub.execute_input":"2025-10-26T13:51:05.771438Z","iopub.status.idle":"2025-10-26T13:52:17.574885Z","shell.execute_reply.started":"2025-10-26T13:51:05.771405Z","shell.execute_reply":"2025-10-26T13:52:17.574243Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-10-26 13:51:22.978318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761486683.193944      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761486683.250756      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b5f837c1a254a9d995034e77d76e98e"}},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from torchinfo import summary\nimport torch.nn as nn\nsummary(model, depth=9, col_names=[\"kernel_size\", \"num_params\", \"trainable\"])\n\nprint(\"\\nDetailed Linear Layers:\\n\" + \"=\"*40)\nfor name, module in model.named_modules():\n    if isinstance(module, nn.Linear):\n        d, k = module.weight.shape\n        print(f\"{name:<70}  {d:>5} x {k:<5}  = {d*k:,} params\")\n        \nprint(\"\\nTotal trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T14:17:02.908094Z","iopub.execute_input":"2025-10-26T14:17:02.908372Z","iopub.status.idle":"2025-10-26T14:17:02.934439Z","shell.execute_reply.started":"2025-10-26T14:17:02.908350Z","shell.execute_reply":"2025-10-26T14:17:02.933498Z"}},"outputs":[{"name":"stdout","text":"\nDetailed Linear Layers:\n========================================\nmodel.layers.0.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.0.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.0.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.0.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.0.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\nmodel.layers.0.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\nmodel.layers.0.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\nmodel.layers.1.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.1.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.1.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.1.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.1.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\nmodel.layers.1.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\nmodel.layers.1.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\nmodel.layers.2.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.2.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.2.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.2.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.2.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\nmodel.layers.2.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\nmodel.layers.2.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\nmodel.layers.3.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.3.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.3.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.3.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.3.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\nmodel.layers.3.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\nmodel.layers.3.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\nmodel.layers.4.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.4.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.4.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.4.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.4.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\nmodel.layers.4.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\nmodel.layers.4.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\nmodel.layers.5.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.5.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.5.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.5.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.5.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\nmodel.layers.5.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\nmodel.layers.5.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\nmodel.layers.6.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.6.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.6.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.6.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.6.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\nmodel.layers.6.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\nmodel.layers.6.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\nmodel.layers.7.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.7.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.7.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.7.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.7.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\nmodel.layers.7.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\nmodel.layers.7.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\nmodel.layers.8.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.8.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.8.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.8.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.8.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\nmodel.layers.8.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\nmodel.layers.8.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\nmodel.layers.9.self_attn.q_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.9.self_attn.k_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.9.self_attn.v_proj                                          1024 x 3072   = 3,145,728 params\nmodel.layers.9.self_attn.o_proj                                          3072 x 3072   = 9,437,184 params\nmodel.layers.9.mlp.gate_proj                                             8192 x 3072   = 25,165,824 params\nmodel.layers.9.mlp.up_proj                                               8192 x 3072   = 25,165,824 params\nmodel.layers.9.mlp.down_proj                                             3072 x 8192   = 25,165,824 params\nmodel.layers.10.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.10.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.10.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.10.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.10.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\nmodel.layers.10.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\nmodel.layers.10.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\nmodel.layers.11.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.11.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.11.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.11.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.11.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\nmodel.layers.11.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\nmodel.layers.11.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\nmodel.layers.12.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.12.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.12.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.12.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.12.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\nmodel.layers.12.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\nmodel.layers.12.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\nmodel.layers.13.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.13.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.13.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.13.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.13.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\nmodel.layers.13.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\nmodel.layers.13.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\nmodel.layers.14.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.14.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.14.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.14.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.14.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\nmodel.layers.14.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\nmodel.layers.14.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\nmodel.layers.15.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.15.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.15.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.15.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.15.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\nmodel.layers.15.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\nmodel.layers.15.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\nmodel.layers.16.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.16.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.16.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.16.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.16.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\nmodel.layers.16.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\nmodel.layers.16.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\nmodel.layers.17.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.17.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.17.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.17.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.17.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\nmodel.layers.17.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\nmodel.layers.17.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\nmodel.layers.18.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.18.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.18.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.18.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.18.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\nmodel.layers.18.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\nmodel.layers.18.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\nmodel.layers.19.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.19.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.19.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.19.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.19.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\nmodel.layers.19.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\nmodel.layers.19.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\nmodel.layers.20.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.20.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.20.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.20.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.20.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\nmodel.layers.20.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\nmodel.layers.20.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\nmodel.layers.21.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.21.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.21.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.21.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.21.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\nmodel.layers.21.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\nmodel.layers.21.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\nmodel.layers.22.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.22.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.22.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.22.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.22.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\nmodel.layers.22.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\nmodel.layers.22.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\nmodel.layers.23.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.23.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.23.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.23.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.23.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\nmodel.layers.23.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\nmodel.layers.23.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\nmodel.layers.24.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.24.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.24.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.24.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.24.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\nmodel.layers.24.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\nmodel.layers.24.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\nmodel.layers.25.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.25.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.25.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.25.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.25.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\nmodel.layers.25.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\nmodel.layers.25.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\nmodel.layers.26.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.26.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.26.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.26.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.26.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\nmodel.layers.26.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\nmodel.layers.26.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\nmodel.layers.27.self_attn.q_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.27.self_attn.k_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.27.self_attn.v_proj                                         1024 x 3072   = 3,145,728 params\nmodel.layers.27.self_attn.o_proj                                         3072 x 3072   = 9,437,184 params\nmodel.layers.27.mlp.gate_proj                                            8192 x 3072   = 25,165,824 params\nmodel.layers.27.mlp.up_proj                                              8192 x 3072   = 25,165,824 params\nmodel.layers.27.mlp.down_proj                                            3072 x 8192   = 25,165,824 params\nlm_head                                                                 128256 x 3072   = 394,002,432 params\n\nTotal trainable parameters: 3212749824\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torch\nfrom prettytable import PrettyTable\n\n# Get one transformer block (acts like one encoder/decoder layer)\nblock = model.model.layers[0]\n\nprint(\"=== TRANSFORMER BLOCK STRUCTURE ===\")\nprint(block)\n\n# Function to recursively list modules and parameter counts\ndef describe_module(module, prefix=\"\"):\n    rows = []\n    total_params = 0\n    for name, child in module.named_children():\n        params = sum(p.numel() for p in child.parameters())\n        total_params += params\n        rows.append([prefix + name, child.__class__.__name__, f\"{params:,}\"])\n        rows += describe_module(child, prefix + \"  \")\n    return rows\n\nrows = describe_module(block)\n\n# Pretty print the architecture\ntable = PrettyTable()\ntable.field_names = [\"Layer Name\", \"Type\", \"Parameters\"]\nfor r in rows:\n    table.add_row(r)\nprint(table)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T14:07:03.278371Z","iopub.execute_input":"2025-10-26T14:07:03.278685Z","iopub.status.idle":"2025-10-26T14:07:03.320326Z","shell.execute_reply.started":"2025-10-26T14:07:03.278661Z","shell.execute_reply":"2025-10-26T14:07:03.319701Z"}},"outputs":[{"name":"stdout","text":"=== TRANSFORMER BLOCK STRUCTURE ===\nLlamaDecoderLayer(\n  (self_attn): LlamaAttention(\n    (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n    (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n    (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n  )\n  (mlp): LlamaMLP(\n    (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n    (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n    (act_fn): SiLU()\n  )\n  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n)\n+--------------------------+----------------+------------+\n|        Layer Name        |      Type      | Parameters |\n+--------------------------+----------------+------------+\n|        self_attn         | LlamaAttention | 25,165,824 |\n|           q_proj         |     Linear     | 9,437,184  |\n|           k_proj         |     Linear     | 3,145,728  |\n|           v_proj         |     Linear     | 3,145,728  |\n|           o_proj         |     Linear     | 9,437,184  |\n|           mlp            |    LlamaMLP    | 75,497,472 |\n|         gate_proj        |     Linear     | 25,165,824 |\n|          up_proj         |     Linear     | 25,165,824 |\n|         down_proj        |     Linear     | 25,165,824 |\n|           act_fn         |      SiLU      |     0      |\n|     input_layernorm      |  LlamaRMSNorm  |   3,072    |\n| post_attention_layernorm |  LlamaRMSNorm  |   3,072    |\n+--------------------------+----------------+------------+\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"num_layers = len(model.model.layers)\nprint(f\"Number of decoder layers: {num_layers}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T14:07:51.626356Z","iopub.execute_input":"2025-10-26T14:07:51.626970Z","iopub.status.idle":"2025-10-26T14:07:51.631066Z","shell.execute_reply.started":"2025-10-26T14:07:51.626945Z","shell.execute_reply":"2025-10-26T14:07:51.630162Z"}},"outputs":[{"name":"stdout","text":"Number of decoder layers: 28\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nimport sys\nimport json\nimport glob\nfrom pathlib import Path\nimport torch\n\nprint(\"Python:\", sys.version.splitlines()[0])\nprint(\"Torch:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"CUDA device:\", torch.cuda.get_device_name(0))\nelse:\n    print(\"Warning: CUDA not available. QLoRA requires GPU for practical training.\")\n\nimport transformers, datasets, peft, bitsandbytes, accelerate\nprint(\"transformers:\", transformers.__version__)\nprint(\"datasets:\", datasets.__version__)\nprint(\"peft:\", peft.__version__)\nimport bitsandbytes as bnb\nprint(\"bitsandbytes:\", bnb.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T21:09:21.841681Z","iopub.execute_input":"2025-10-23T21:09:21.842554Z","iopub.status.idle":"2025-10-23T21:09:56.681810Z","shell.execute_reply.started":"2025-10-23T21:09:21.842513Z","shell.execute_reply":"2025-10-23T21:09:56.680724Z"}},"outputs":[{"name":"stdout","text":"Python: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\nTorch: 2.6.0+cu124\nCUDA available: True\nCUDA device: Tesla T4\n","output_type":"stream"},{"name":"stderr","text":"2025-10-23 21:09:38.220712: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761253778.452965      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761253778.520890      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"transformers: 4.53.3\ndatasets: 4.1.1\npeft: 0.16.0\nbitsandbytes: 0.48.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/finance-finetuning-dataset-combined\"\nBASE_MODEL_PATH = \"/kaggle/input/llama-3.2-3b-instruct/pytorch/default/1\"\nOUTPUT_DIR = \"/kaggle/working/qlora_lora_finance\"  # where adapters/checkpoints will be saved\n\n# Training hyperparameters\nMICRO_BATCH_SIZE = 4           \nGRAD_ACCUM_STEPS = 8           \nEPOCHS = 3\nLEARNING_RATE = 2e-4\nCUTOFF_LEN = 1024              # token sequence length (common for LLaMA-like models)\nLORA_R = 8\nLORA_ALPHA = 32\nLORA_DROPOUT = 0.05\nSEED = 42\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(\"DATA_PATH:\", DATA_PATH)\nprint(\"BASE_MODEL_PATH:\", BASE_MODEL_PATH)\nprint(\"OUTPUT_DIR:\", OUTPUT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T21:12:06.093171Z","iopub.execute_input":"2025-10-23T21:12:06.093992Z","iopub.status.idle":"2025-10-23T21:12:06.100523Z","shell.execute_reply.started":"2025-10-23T21:12:06.093965Z","shell.execute_reply":"2025-10-23T21:12:06.099640Z"}},"outputs":[{"name":"stdout","text":"DATA_PATH: /kaggle/input/finance-finetuning-dataset-combined\nBASE_MODEL_PATH: /kaggle/input/llama-3.2-3b-instruct/pytorch/default/1\nOUTPUT_DIR: /kaggle/working/qlora_lora_finance\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load tokenizer - prefer local model path; fallback to hub if needed\nprint(\"Loading tokenizer from:\", BASE_MODEL_PATH)\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, use_fast=False, trust_remote_code=True)\n# Some LLaMA tokenizers do not have pad_token; set it to eos_token for causal LM training\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# BitsAndBytes 4-bit config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nprint(\"Loading base model (4-bit) — this can take a while.\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_PATH,\n    device_map=\"auto\",\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n)\n\n# Freeze base model parameters\nfor param in model.parameters():\n    param.requires_grad = False\n\nprint(\"Model loaded. Trainable params initially:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T21:12:15.537402Z","iopub.execute_input":"2025-10-23T21:12:15.538145Z","iopub.status.idle":"2025-10-23T21:12:55.335006Z","shell.execute_reply.started":"2025-10-23T21:12:15.538108Z","shell.execute_reply":"2025-10-23T21:12:55.334134Z"}},"outputs":[{"name":"stdout","text":"Loading tokenizer from: /kaggle/input/llama-3.2-3b-instruct/pytorch/default/1\nLoading base model (4-bit) — this can take a while.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ac6dede97924882af27a69bc7edd82e"}},"metadata":{}},{"name":"stdout","text":"Model loaded. Trainable params initially: 0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n# Prepare the model for k-bit training (adjusts some layers / casts)\nmodel = prepare_model_for_kbit_training(model)\n\n# PEFT will ignore them. Typical attention projection names for LLaMA-like models:\ntarget_modules = [\n    \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n    \"gate_proj\", \"down_proj\", \"up_proj\",\n    \"wi\", \"wo\", \"wq\", \"wv\", \"wk\", \"wo\"\n]\n\nlora_config = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=target_modules,\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()  # show how many params will be trained\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T21:13:55.029515Z","iopub.execute_input":"2025-10-23T21:13:55.030267Z","iopub.status.idle":"2025-10-23T21:13:55.970188Z","shell.execute_reply.started":"2025-10-23T21:13:55.030238Z","shell.execute_reply":"2025-10-23T21:13:55.969212Z"}},"outputs":[{"name":"stdout","text":"trainable params: 12,156,928 || all params: 3,224,906,752 || trainable%: 0.3770\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from datasets import load_dataset, Dataset, DatasetDict\n\ndef find_data_files(path):\n    p = Path(path)\n    if p.is_file():\n        return [str(p)]\n    # If it's a dir, look for .jsonl/.json/.csv\n    files = []\n    for ext in (\"*.jsonl\",\"*.json\",\"*.csv\"):\n        files.extend(sorted([str(x) for x in p.glob(ext)]))\n    return files\n\ndata_files = find_data_files(DATA_PATH)\nif len(data_files) == 0:\n    raise FileNotFoundError(f\"No dataset files found at {DATA_PATH}. Expect .jsonl/.json/.csv or a file path.\")\nprint(\"Found data files:\", data_files[:10])\n\n# The dataset has fields: instruction, input, output (answer)\nsample_file = data_files[0]\nif sample_file.endswith(\".csv\"):\n    raw = load_dataset(\"csv\", data_files=data_files)\nelse:\n    # try json (datasets handles jsonl too)\n    raw = load_dataset(\"json\", data_files=data_files)\n\n# If the dataset has splits, unify to train split\nif \"train\" in raw:\n    ds = raw[\"train\"]\nelse:\n    # raw could be a single split\n    split_name = list(raw.keys())[0]\n    ds = raw[split_name]\n\nprint(\"Dataset size:\", len(ds))\nprint(\"Some columns:\", ds.column_names)\nprint(\"Sample record:\", ds[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T21:14:15.165177Z","iopub.execute_input":"2025-10-23T21:14:15.165527Z","iopub.status.idle":"2025-10-23T21:14:15.767139Z","shell.execute_reply.started":"2025-10-23T21:14:15.165503Z","shell.execute_reply":"2025-10-23T21:14:15.766372Z"}},"outputs":[{"name":"stdout","text":"Found data files: ['/kaggle/input/finance-finetuning-dataset-combined/Finance_finetuning_dataset_combined.jsonl']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b039dc9cd34c45ce91173d16597cda04"}},"metadata":{}},{"name":"stdout","text":"Dataset size: 20083\nSome columns: ['instruction', 'input', 'output', 'question', 'answer']\nSample record: {'instruction': '<title>Minority Depository Institution (MDI: Meaning, Benefits, History</title>\\n\"Semiannual Report to the Congress: April 1, 2019 - September 30, 2019,\" Page 10.\\nFederal Deposit Insurance Corporation. \"2019 Minority Depository Institutions: Structure, Performance, and Social Impact,\" Page 55.\\nFederal Deposit Insurance Corporation. \"Preservation and Promotion of Minority Depository Institutions.\"\\nFederal Reserve System. \"Preserving Minority Depository Institutions - May 2020.\"\\nFederal Register. \"Minority Depository Institution Preservation Program.\"', 'input': 'market+news', 'output': 'The given text passages discuss Minority Depository Institutions.', 'question': None, 'answer': None}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import math\nfrom transformers import PreTrainedTokenizerBase\n\nPROMPT_TEMPLATE = (\n    \"### Instruction:\\n{instruction}\\n\\n\"\n    \"### Input:\\n{input}\\n\\n\"\n    \"### Response:\\n\"\n)\n\ndef make_prompt(example):\n    instruction = example.get(\"instruction\") or example.get(\"Instruction\") or example.get(\"prompt\") or \"\"\n    inp = example.get(\"input\") or example.get(\"Input\") or \"\"\n    # If dataset uses 'output' or 'answer' keys:\n    output = example.get(\"output\") or example.get(\"Output\") or example.get(\"answer\") or example.get(\"Answer\") or example.get(\"response\") or \"\"\n    prompt = PROMPT_TEMPLATE.format(instruction=instruction.strip(), input=inp.strip())\n    full = prompt + output.strip()\n    return prompt, full, output\n\ndef tokenize_function(example, tokenizer: PreTrainedTokenizerBase):\n    prompt, full, output = make_prompt(example)\n    # tokenize prompt and full separately to find token lengths\n    tokenized_full = tokenizer(full, truncation=True, max_length=CUTOFF_LEN, padding=False)\n    tokenized_prompt = tokenizer(prompt, truncation=True, max_length=CUTOFF_LEN, padding=False)\n    input_ids = tokenized_full[\"input_ids\"]\n    prompt_len = len(tokenized_prompt[\"input_ids\"])\n    labels = input_ids.copy()\n    # mask prompt tokens - so loss is computed only on the response portion\n    labels[:prompt_len] = [-100] * prompt_len\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": tokenized_full[\"attention_mask\"],\n        \"labels\": labels,\n    }\n\n# Quick tokenization test on a small sample\ntok_test = tokenize_function(ds[0], tokenizer)\nprint(\"tokenized sample lengths:\", {k: len(v) for k,v in tok_test.items()})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T21:15:09.516046Z","iopub.execute_input":"2025-10-23T21:15:09.516952Z","iopub.status.idle":"2025-10-23T21:15:09.531483Z","shell.execute_reply.started":"2025-10-23T21:15:09.516902Z","shell.execute_reply":"2025-10-23T21:15:09.530789Z"}},"outputs":[{"name":"stdout","text":"tokenized sample lengths: {'input_ids': 141, 'attention_mask': 141, 'labels': 141}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Shuffle + split\nds = ds.shuffle(seed=SEED)\n\n# small validation split: 2-5% (adjust as dataset size)\nval_size = max(1, int(0.02 * len(ds)))\ntrain_ds = ds.select(range(len(ds) - val_size))\nval_ds = ds.select(range(len(ds) - val_size, len(ds)))\n\nprint(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds))\n\n# Tokenize with batched mapping\ndef batched_tokenize(batch):\n    results = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n    for i in range(len(batch[next(iter(batch.keys()))])):  # iterate rows\n        ex = {k: batch[k][i] for k in batch.keys()}\n        tok = tokenize_function(ex, tokenizer)\n        results[\"input_ids\"].append(tok[\"input_ids\"])\n        results[\"attention_mask\"].append(tok[\"attention_mask\"])\n        results[\"labels\"].append(tok[\"labels\"])\n    return results\n\n# Use batched mapping\ntrain_tok = train_ds.map(batched_tokenize, batched=True, remove_columns=train_ds.column_names)\nval_tok = val_ds.map(batched_tokenize, batched=True, remove_columns=val_ds.column_names)\n\nprint(\"Tokenized train columns:\", train_tok.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T21:15:19.044344Z","iopub.execute_input":"2025-10-23T21:15:19.045124Z","iopub.status.idle":"2025-10-23T21:15:43.540560Z","shell.execute_reply.started":"2025-10-23T21:15:19.045097Z","shell.execute_reply":"2025-10-23T21:15:43.539781Z"}},"outputs":[{"name":"stdout","text":"Train size: 19682 Val size: 401\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19682 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f313ff645034d32b6b958876800d31f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/401 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a1a242615f246cc8e88a7cf19920537"}},"metadata":{}},{"name":"stdout","text":"Tokenized train columns: ['input_ids', 'attention_mask', 'labels']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Dict, List\nimport torch\nfrom transformers.tokenization_utils_base import PaddingStrategy\n\n@dataclass\nclass DataCollatorForCausalLM:\n    tokenizer: PreTrainedTokenizerBase\n    padding: bool = True\n    max_length: int = CUTOFF_LEN\n\n    def __call__(self, batch: List[Dict[str, List[int]]]) -> Dict[str, torch.Tensor]:\n        input_ids = [torch.tensor(b[\"input_ids\"], dtype=torch.long) for b in batch]\n        attention_mask = [torch.tensor(b[\"attention_mask\"], dtype=torch.long) for b in batch]\n        labels = [torch.tensor(b[\"labels\"], dtype=torch.long) for b in batch]\n        # pad using tokenizer.pad\n        pad_token_id = self.tokenizer.pad_token_id\n        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=pad_token_id)\n        attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n\ndata_collator = DataCollatorForCausalLM(tokenizer=tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T21:16:29.875531Z","iopub.execute_input":"2025-10-23T21:16:29.875932Z","iopub.status.idle":"2025-10-23T21:16:29.884353Z","shell.execute_reply.started":"2025-10-23T21:16:29.875911Z","shell.execute_reply":"2025-10-23T21:16:29.883376Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\nimport inspect\nimport torch\n\n# Prepare the desired kwargs (our canonical set)\ndesired_args = dict(\n    output_dir=OUTPUT_DIR,\n    per_device_train_batch_size=MICRO_BATCH_SIZE,\n    per_device_eval_batch_size=MICRO_BATCH_SIZE,\n    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n    num_train_epochs=EPOCHS,\n    learning_rate=LEARNING_RATE,\n    fp16=(True if torch.cuda.is_available() else False),\n    logging_steps=50,\n    evaluation_strategy=\"steps\",  # may be accepted or not depending on transformers version\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=500,\n    save_total_limit=3,\n    load_best_model_at_end=False,\n    report_to=\"none\",\n    # ddp_find_unused_parameters only relevant in some versions; will be filtered if unsupported\n    ddp_find_unused_parameters=(False if torch.cuda.is_available() else None),\n)\n\n# Inspect TrainingArguments signature and keep only supported kwargs\nsig = inspect.signature(TrainingArguments.__init__)\nsupported_params = set(sig.parameters.keys())\n# remove 'self' if present\nsupported_params.discard('self')\n\n# Filter out unsupported args and None values (some versions don't accept None for ddp flag)\nfiltered_args = {k: v for k, v in desired_args.items() if (k in supported_params and v is not None)}\n\nprint(\"transformers version:\", __import__(\"transformers\").__version__)\nprint(\"TrainingArguments supports these params:\", sorted(supported_params))\nprint(\"Using the following training args (filtered):\")\nfor k, v in filtered_args.items():\n    print(f\"  {k}: {v}\")\n\n# Create TrainingArguments with the safe filtered args\ntraining_args = TrainingArguments(**filtered_args)\n\n# Create Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_tok,\n    eval_dataset=val_tok,\n    data_collator=data_collator,\n)\n\nprint(\"Trainer created successfully. Starting training...\")\n\ntrainer.train()\n\n# After training, save adapters and tokenizer\nprint(\"Saving LoRA adapters and tokenizer to:\", OUTPUT_DIR)\nmodel.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T21:24:11.375559Z","iopub.execute_input":"2025-10-23T21:24:11.376004Z","iopub.status.idle":"2025-10-23T21:33:59.607636Z","shell.execute_reply.started":"2025-10-23T21:24:11.375977Z","shell.execute_reply":"2025-10-23T21:33:59.606312Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"transformers version: 4.53.3\nTrainingArguments supports these params: ['accelerator_config', 'adafactor', 'adam_beta1', 'adam_beta2', 'adam_epsilon', 'auto_find_batch_size', 'average_tokens_across_devices', 'batch_eval_metrics', 'bf16', 'bf16_full_eval', 'data_seed', 'dataloader_drop_last', 'dataloader_num_workers', 'dataloader_persistent_workers', 'dataloader_pin_memory', 'dataloader_prefetch_factor', 'ddp_backend', 'ddp_broadcast_buffers', 'ddp_bucket_cap_mb', 'ddp_find_unused_parameters', 'ddp_timeout', 'debug', 'deepspeed', 'disable_tqdm', 'do_eval', 'do_predict', 'do_train', 'eval_accumulation_steps', 'eval_delay', 'eval_do_concat_batches', 'eval_on_start', 'eval_steps', 'eval_strategy', 'eval_use_gather_object', 'fp16', 'fp16_backend', 'fp16_full_eval', 'fp16_opt_level', 'fsdp', 'fsdp_config', 'fsdp_min_num_params', 'fsdp_transformer_layer_cls_to_wrap', 'full_determinism', 'gradient_accumulation_steps', 'gradient_checkpointing', 'gradient_checkpointing_kwargs', 'greater_is_better', 'group_by_length', 'half_precision_backend', 'hub_always_push', 'hub_model_id', 'hub_private_repo', 'hub_revision', 'hub_strategy', 'hub_token', 'ignore_data_skip', 'include_for_metrics', 'include_inputs_for_metrics', 'include_num_input_tokens_seen', 'include_tokens_per_second', 'jit_mode_eval', 'label_names', 'label_smoothing_factor', 'learning_rate', 'length_column_name', 'liger_kernel_config', 'load_best_model_at_end', 'local_rank', 'log_level', 'log_level_replica', 'log_on_each_node', 'logging_dir', 'logging_first_step', 'logging_nan_inf_filter', 'logging_steps', 'logging_strategy', 'lr_scheduler_kwargs', 'lr_scheduler_type', 'max_grad_norm', 'max_steps', 'metric_for_best_model', 'mp_parameters', 'neftune_noise_alpha', 'no_cuda', 'num_train_epochs', 'optim', 'optim_args', 'optim_target_modules', 'output_dir', 'overwrite_output_dir', 'past_index', 'per_device_eval_batch_size', 'per_device_train_batch_size', 'per_gpu_eval_batch_size', 'per_gpu_train_batch_size', 'prediction_loss_only', 'push_to_hub', 'push_to_hub_model_id', 'push_to_hub_organization', 'push_to_hub_token', 'ray_scope', 'remove_unused_columns', 'report_to', 'restore_callback_states_from_checkpoint', 'resume_from_checkpoint', 'run_name', 'save_on_each_node', 'save_only_model', 'save_safetensors', 'save_steps', 'save_strategy', 'save_total_limit', 'seed', 'skip_memory_metrics', 'tf32', 'torch_compile', 'torch_compile_backend', 'torch_compile_mode', 'torch_empty_cache_steps', 'torchdynamo', 'tpu_metrics_debug', 'tpu_num_cores', 'use_cpu', 'use_ipex', 'use_legacy_prediction_loop', 'use_liger_kernel', 'use_mps_device', 'warmup_ratio', 'warmup_steps', 'weight_decay']\nUsing the following training args (filtered):\n  output_dir: /kaggle/working/qlora_lora_finance\n  per_device_train_batch_size: 4\n  per_device_eval_batch_size: 4\n  gradient_accumulation_steps: 8\n  num_train_epochs: 3\n  learning_rate: 0.0002\n  fp16: True\n  logging_steps: 50\n  eval_steps: 500\n  save_strategy: steps\n  save_steps: 500\n  save_total_limit: 3\n  load_best_model_at_end: False\n  report_to: none\n  ddp_find_unused_parameters: False\nTrainer created successfully. Starting training...\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='31' max='1848' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  31/1848 09:30 < 9:55:35, 0.05 it/s, Epoch 0.05/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/3459166130.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Start training (no resume arg here; if you need to resume, use Trainer API for your transformers version)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# After training, save adapters and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2207\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2546\u001b[0m                     )\n\u001b[1;32m   2547\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2548\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3795\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3797\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3799\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2572\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2573\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2574\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2575\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2576\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"from peft import PeftModel\n\nprint(\"Loading base model and applying LoRA adapters for inference...\")\nbase = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_PATH,\n    device_map=\"auto\",\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n)\n\n# Load LoRA weights (this will attach them to base model)\nfrom peft import PeftModel\nlora_model = PeftModel.from_pretrained(base, OUTPUT_DIR, torch_dtype=torch.float16)\nlora_model.eval()\n\n# Example inference\ndef generate(prompt, max_new_tokens=256, temperature=0.2, top_p=0.95):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(lora_model.device)\n    with torch.no_grad():\n        out = lora_model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=temperature,\n            top_p=top_p,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n        )\n    return tokenizer.decode(out[0], skip_special_tokens=True)\n\n# Test with a small prompt:\nsample_prompt = PROMPT_TEMPLATE.format(instruction=\"Explain what a dividend yield is\", input=\"\")\nprint(generate(sample_prompt, max_new_tokens=120))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}