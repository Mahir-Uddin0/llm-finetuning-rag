{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13466316,"sourceType":"datasetVersion","datasetId":8548198},{"sourceId":621070,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":467088,"modelId":482912}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q langchain faiss-cpu unstructured PyPDF2\n!pip install -q huggingface_hub\n!pip install -U langchain-community langchain-huggingface\n!pip install -q langchain-huggingface\n!pip install transformers datasets tqdm ddgs\n!pip install langchain-embedding\n!pip install InstructorEmbedding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:58:59.475053Z","iopub.execute_input":"2025-10-27T21:58:59.475397Z","iopub.status.idle":"2025-10-27T21:59:26.607763Z","shell.execute_reply.started":"2025-10-27T21:58:59.475371Z","shell.execute_reply":"2025-10-27T21:59:26.606714Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.8/449.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain-classic 1.0.0 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.3.79 which is incompatible.\nlangchain-classic 1.0.0 requires langchain-text-splitters<2.0.0,>=1.0.0, but you have langchain-text-splitters 0.3.11 which is incompatible.\nlangchain-huggingface 1.0.0 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.3.79 which is incompatible.\nlangchain-community 0.4.1 requires langchain-core<2.0.0,>=1.0.1, but you have langchain-core 0.3.79 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.4.1)\nRequirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (1.0.0)\nCollecting langchain-core<2.0.0,>=1.0.1 (from langchain-community)\n  Using cached langchain_core-1.0.1-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.0.0)\nRequirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\nRequirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.5)\nRequirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.3)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\nRequirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.11.0)\nRequirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\nRequirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.36.0)\nRequirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.19.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (25.0)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.1.10)\nCollecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n  Using cached langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.0a1)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.0)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2.4.1)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.8.3)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.37.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.37.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain-community) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\nUsing cached langchain_core-1.0.1-py3-none-any.whl (467 kB)\nUsing cached langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\nInstalling collected packages: langchain-core, langchain-text-splitters\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.79\n    Uninstalling langchain-core-0.3.79:\n      Successfully uninstalled langchain-core-0.3.79\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.11\n    Uninstalling langchain-text-splitters-0.3.11:\n      Successfully uninstalled langchain-text-splitters-0.3.11\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.1 which is incompatible.\nlangchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-core-1.0.1 langchain-text-splitters-1.0.0\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.1.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: ddgs in /usr/local/lib/python3.11/dist-packages (9.6.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from ddgs) (8.3.0)\nRequirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from ddgs) (0.15.0)\nRequirement already satisfied: lxml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from ddgs) (6.0.2)\nRequirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.28.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (2025.8.3)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.10)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\nRequirement already satisfied: brotli in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.1.0)\nRequirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.3.0)\nRequirement already satisfied: socksio==1.* in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (6.1.0)\nRequirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.1.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.3.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-embedding (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-embedding\u001b[0m\u001b[31m\n\u001b[0mCollecting InstructorEmbedding\n  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl.metadata (20 kB)\nDownloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\nInstalling collected packages: InstructorEmbedding\nSuccessfully installed InstructorEmbedding-1.0.1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport json\nfrom tqdm import tqdm\nfrom langchain.vectorstores import FAISS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:56:18.864286Z","iopub.execute_input":"2025-10-27T21:56:18.864678Z","iopub.status.idle":"2025-10-27T21:56:57.076629Z","shell.execute_reply.started":"2025-10-27T21:56:18.864654Z","shell.execute_reply":"2025-10-27T21:56:57.075703Z"}},"outputs":[{"name":"stderr","text":"2025-10-27 21:56:37.174497: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761602197.393423      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761602197.454811      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Model Load","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom langchain.llms import HuggingFacePipeline\nfrom InstructorEmbedding import INSTRUCTOR\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# === 1️⃣ Load fine-tuned financial LLaMA3 model for .invoke() ===\nllm_model_path = \"/kaggle/input/investing-fine-tuned-model-llama3-2/other/default/1\"\nllm_tokenizer = AutoTokenizer.from_pretrained(llm_model_path)\nllm_model = AutoModelForCausalLM.from_pretrained(llm_model_path, torch_dtype=torch.float16, device_map=\"auto\")\n\nllm_pipeline = pipeline(\n    \"text-generation\",\n    model=llm_model,\n    tokenizer=llm_tokenizer,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    max_new_tokens=256,\n    temperature=0.2,\n    do_sample=False\n)\nllm = HuggingFacePipeline(pipeline=llm_pipeline)\nprint(\"✅ Fine-tuned financial model loaded.\")\n\n# === 2️⃣ Load local INSTRUCTOR embedding model ===\nembedding_model_path = \"/kaggle/input/qwen-3-embedding/transformers/4b/1\"\nembeddings_model = INSTRUCTOR(embedding_model_path, device=device)\nprint(\"✅ Embedding model loaded successfully.\")\n\n# LLM\nresponse = llm.invoke(\"Get Tesla 2023 annual report\")\nprint(response)\n\n# Embeddings\nvector = embeddings_model.encode(\"Tesla revenue 2023\")\nprint(len(vector))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:59:26.609453Z","iopub.execute_input":"2025-10-27T21:59:26.609754Z","iopub.status.idle":"2025-10-27T22:03:17.567599Z","shell.execute_reply.started":"2025-10-27T21:59:26.609720Z","shell.execute_reply":"2025-10-27T22:03:17.566469Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"021708842f35405ca43d9f38e7dedf66"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n/tmp/ipykernel_37/2433458544.py:24: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFacePipeline``.\n  llm = HuggingFacePipeline(pipeline=llm_pipeline)\n","output_type":"stream"},{"name":"stdout","text":"✅ Fine-tuned financial model loaded.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75446e73c58d4a1998f720bb731d0104"}},"metadata":{}},{"name":"stderr","text":"`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True}. If this is not desired, please set these values explicitly.\n","output_type":"stream"},{"name":"stdout","text":"✅ Embedding model loaded successfully.\nGet Tesla 2023 annual report\nThe Tesla 2023 annual report is available. It covers the company's financial performance, product offerings, and future plans. It also includes information about the company's commitment to sustainability and its efforts to reduce environmental impact. The report is available in PDF format. (Note: The exact date of the report's release is not provided in the given text.) The report is also available in digital format, but the exact date of its release is not provided in the given text. (Note: The exact date of the report's release is not provided in the given text.) The report is also available in digital format, but the exact date of its release is not provided in the given text.) The report is also available in digital format, but the exact date of its release is not provided in the given text.) The report is also available in digital format, but the exact date of its release is not provided in the given text.) The report is also available in digital format, but the exact date of its release is not provided in the given text.) The report is also available in digital format, but the exact date of its release is not provided in the given text.) The report is also available in digital format, but the exact date of its release is not provided in the\n2560\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Common Functions","metadata":{}},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.schema import Document\nfrom typing import List\n\ndef get_split_docs(documents):\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=2000,\n        chunk_overlap=350\n    )\n    split_docs = text_splitter.split_documents(documents)\n    print(f\"Created {len(split_docs)} text chunks.\")\n    return split_docs\n\n\n# Example documents\ndocs = [\n    Document(page_content=\"This is the content of document 1.\"),\n    Document(page_content=\"This is the content of document 2.\")\n]\n\nchunks = split_text(docs)\nprint(chunks[0].page_content)  # first chunk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:52:50.452576Z","iopub.execute_input":"2025-10-27T20:52:50.454211Z","iopub.status.idle":"2025-10-27T20:52:51.267773Z","shell.execute_reply.started":"2025-10-27T20:52:50.454155Z","shell.execute_reply":"2025-10-27T20:52:51.266751Z"}},"outputs":[{"name":"stdout","text":"Created 2 text chunks.\nThis is the content of document 1.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def get_combined_vector_db(*args):\n    combined_docs = []\n    for doc_list in args:\n        if doc_list:  # make sure it's not None or empty\n            combined_docs.extend(doc_list)\n    \n    # Create vector DB for all docs\n    combined_vector_db = FAISS.from_documents(combined_docs, embeddings)\n    \n    return combined_vector_db\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_pdf_split_docs(pdf_folder):\n    pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n    all_documents = []\n    \n    for pdf_path in pdf_files:\n        loader = PyPDFLoader(pdf_path)\n        docs = loader.load()  # this returns list of Document objects\n        all_documents.extend(docs)\n        \n    split_docs = get_split_docs(all_documents)\n    \n    return split_docs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Three sources of information will be used. \n### 1. PDF books added to Kaggle input directory\n### 2. Web search results\n### 3. Company Annual Reports","metadata":{}},{"cell_type":"markdown","source":"# 1. PDF Embedding","metadata":{}},{"cell_type":"code","source":"pdf_folder = \"/kaggle/input/investing-books-pdf\"\nlocal_pdf_split_docs = get_pdf_split_docs(pdf_folder)\n\ndef get_local_pdf_split_docs(query):\n    return local_pdf_split_docs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Web Search","metadata":{}},{"cell_type":"code","source":"from ddgs import DDGS\nfrom langchain.schema import Document\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef web_search(query, max_results=3):\n    docs = []\n\n    # Search the web using DuckDuckGo\n    with DDGS() as ddgs:\n        results = list(ddgs.text(query, max_results=max_results))\n\n    for result in results:\n        url = result.get(\"href\")\n        title = result.get(\"title\")\n\n        try:\n            # Fetch the web page content\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n\n            # Parse visible text from HTML\n            soup = BeautifulSoup(response.text, \"html.parser\")\n            for tag in soup([\"script\", \"style\", \"noscript\"]):\n                tag.extract()\n            text = \" \".join(soup.get_text(separator=\" \").split())\n\n            # Only keep if text length is reasonable\n            if len(text) > 500:\n                docs.append(Document(page_content=text, metadata={\"source\": url, \"title\": title}))\n        except Exception as e:\n            # print(f\"⚠️ Skipping {url}: {e}\")\n\n    # print(f\"✅ Retrieved {len(docs)} web documents from top {max_results} results.\")\n    return docs\n\nquery = \"Tesla 2025 annual revenue analysis\"\ndocuments = web_search(query)\n\nprint(f\"Number of documents: {len(documents)}\")\nprint(documents[0].metadata)\nprint(documents[0].page_content[:500])  # preview text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:14:06.963444Z","iopub.execute_input":"2025-10-27T21:14:06.966427Z","iopub.status.idle":"2025-10-27T21:14:15.986004Z","shell.execute_reply.started":"2025-10-27T21:14:06.966355Z","shell.execute_reply":"2025-10-27T21:14:15.984458Z"}},"outputs":[{"name":"stdout","text":"✅ Retrieved 3 web documents from top 3 results.\nNumber of documents: 3\n{'source': 'https://stockanalysis.com/stocks/tsla/revenue/', 'title': 'Tesla (TSLA) Revenue 2015-2025'}\nTesla (TSLA) Revenue 2015-2025 Skip to main content Log In Sign Up Home Watchlist Stocks Stock Screener Stock Exchanges Comparison Tool Earnings Calendar By Industry Stock Lists Top Analysts Top Stocks Corporate Actions IPOs Recent IPOs IPO Calendar IPO Statistics IPO News IPO Screener ETFs ETF Screener Comparison Tool New Launches ETF Providers News Trending Articles Technical Chart Market Movers Top Gainers Top Losers Most Active Premarket After Hours Market Heatmap Market Newsletter Stock Ana\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def get_web_search_split_docs(query, max_results=3):\n    docs = web_search(query, max_results)\n    split_docs = get_split_docs(docs)\n    return split_docs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Web PDF Retriever","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nimport json\n\ndef analyze_query_with_llm(query: str):\n    \"\"\"\n    Use the fine-tuned LLM to decide if annual report retrieval is required,\n    and extract the company name and year if applicable.\n\n    Returns a dictionary like:\n    {\n        \"required\": True,\n        \"company\": \"Tesla\",\n        \"year\": \"2024\"\n    }\n    \"\"\"\n    prompt = f\"\"\"\n    You are an intelligent financial assistant. \n    Your task is to analyze the user query and decide:\n    1. Whether it requires downloading a company's annual report.\n    2. If yes, extract the company name and the year of the report.\n    \n    Respond strictly in this JSON format:\n    {{\n      \"required\": true or false,\n      \"company\": \"Company name or null\",\n      \"year\": \"Year or null\"\n    }}\n    \n    User query: \"{query}\"\n    \"\"\"\n    # Tokenize input\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    # Generate model output\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=256,\n        temperature=0.2,\n        do_sample=False\n    )\n\n    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Try to extract JSON response from model output\n    try:\n        json_start = response_text.find(\"{\")\n        json_end = response_text.rfind(\"}\") + 1\n        json_str = response_text[json_start:json_end]\n        result = json.loads(json_str)\n    except Exception as e:\n        print(\"⚠️ Could not parse model output properly. Raw output:\")\n        print(response_text)\n        result = {\"required\": False, \"company\": None, \"year\": None}\n\n    return result\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ddgs import DDGS\n\ndef search_pdfs(company_name, year=2024, max_results=5):\n    query = f\"{company_name} {year} annual report filetype:pdf\"\n    with DDGS() as ddgs:\n        results = list(ddgs.text(query, max_results=max_results))\n    pdf_links = [r['href'] for r in results if r['href'].endswith('.pdf')]\n    return pdf_links\n\n# Example:\npdf_urls = search_pdfs(\"Square Pharma\", 2024)\nprint(pdf_urls)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T19:33:08.667088Z","iopub.status.idle":"2025-10-27T19:33:08.667681Z","shell.execute_reply.started":"2025-10-27T19:33:08.667432Z","shell.execute_reply":"2025-10-27T19:33:08.667453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\n\ndef download_pdf(pdf_links, save_path):\n    for url in pdf_links:    \n        response = requests.get(url, stream=True)\n        if response.status_code == 200 and 'application/pdf' in response.headers.get('Content-Type', ''):\n            with open(save_path, 'wb') as f:\n                for chunk in response.iter_content(1024):\n                    f.write(chunk)\n            print(f\"✅ Downloaded: {save_path}\")\n        else:\n            print(\"❌ Not a valid PDF or download failed.\")\n# if pdf_urls:\n    # for i in range(len(pdf_urls)):\n        # download_pdf(pdf_urls[4], \"apple_2024_annual_report.pdf\")\n# download_pdf(\"https://www.squarepharma.com.bd/downloads/Square%20Pharma_AR_24%20dt-24-11-24_compressed_1.pdf\", \"hjd.pdf\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T19:33:08.670099Z","iopub.status.idle":"2025-10-27T19:33:08.670506Z","shell.execute_reply.started":"2025-10-27T19:33:08.670325Z","shell.execute_reply":"2025-10-27T19:33:08.670343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_web_pdf_split_docs(query, max_results=5):\n    result = analyze_query_with_llm(query) \n    \n    pdf_links = search_pdfs(result['company'], result['year'], max_results)\n    \n    output_dir = \"/kaggle/working/\"\n    \n    download_pdf(pdf_links)\n    \n    split_docs = get_pdf_split_docs(output_dir)\n\n    return split_docs\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Retrieval","metadata":{}},{"cell_type":"code","source":"def rag_retrieval(vector_db, query):\n    \"\"\"\n    Perform similarity search on the vector DB using the query.\n    Returns top relevant documents.\n    \"\"\"\n    return vector_db.similarity_search(query, k=5)  # top 5 results\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Augmentation","metadata":{}},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\n\n# Define the template string\nprompt = \"\"\"\nYou are a financial and investing expert specializing in long-term investing, company analysis, and stock market strategies. \nYou have deep knowledge of business fundamentals, financial statements, market trends, and investment analysis.\n\nUse the following context from company reports, web data, and relevant documents to answer the user query. \nAlways base your answers on the provided context and do not make unsupported claims. \n\nContext:\n{context}\n\nUser Question:\n{question}\n\nInstructions:\n- Analyze the financial and business information carefully.\n- Provide long-term investment insights.\n- Give clear reasoning and avoid generic statements.\n- If the context does not provide sufficient information, say \"Insufficient data provided.\"\n- Summarize your analysis in a professional and concise manner.\n\nAnswer:\n\"\"\"\n\n# Create a PromptTemplate instance\ninvesting_prompt = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=prompt\n)\n\n# Example usage\nretrieved_context = \"Tesla's revenue in 2023 increased by 15% compared to 2022. Gross margin is 25%.\"\nuser_query = \"Should I invest in Tesla for the next 5 years?\"\n\nfinal_prompt = investing_prompt.format(context=retrieved_context, question=user_query)\nprint(final_prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generation","metadata":{}},{"cell_type":"code","source":"query = \"Give me detailed fundamental analysis on Marico Bangladesh Limited\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"context = rag_retrieval(vector_db, query)\nfinal_prompt = investing_prompt.format(context=retrieved_context, question=user_query)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ans = llm.invoke(final_prompt)\nprint(ans)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Chain Construction","metadata":{}},{"cell_type":"code","source":"# from langchain.schema import Document\n# from langchain.runnables import (\n#     RunnablePassthrough,\n#     RunnableLambda,\n#     RunnableSequence,\n#     RunnableParallel,\n# )\n\n# # 2️⃣ Parallel execution of three branches\n# parallel_chain = RunnableParallel(\n#     {\n#         \"books\": RunnableSequence(\n#             [\n#                 RunnableLambda(lambda query: get_local_pdf_split_docs(query)),   # split preloaded book PDFs\n#             ]\n#         ),\n#         \"online_text\": RunnableSequence(\n#             [\n#                 RunnableLambda(lambda query: get_web_search_split_docs(query)),  # search + split text\n#             ]\n#         ),\n#         \"online_pdf\": RunnableSequence(\n#             [\n#                 RunnableLambda(lambda query: get_web_pdf_split_docs(query)),  # download + split PDFs\n#             ]\n#         ),\n#     },\n#     combine_mode=\"list\",  # returns a list of outputs for each branch\n# )\n\n# # 3️⃣ Combine embeddings and build RAG pipeline\n# combine_chain = RunnableSequence(\n#     [\n#         RunnableLambda(lambda split_docs_lists: get_combined_vector_db(*split_docs_lists)),  # combine all split docs\n#         RunnableLambda(lambda vector_db: rag_retrieval(vector_db)),                           # retrieval + augmentation\n#         RunnableLambda(lambda result: generate_answer(result)),                               # LLM generation\n#     ]\n# )\n\n# # 4️⃣ Full pipeline: sequence of parallel + combine\n# full_pipeline = RunnableSequence(\n#     [\n#         RunnablePassthrough(),\n\n#         # Step 2: Parallel execution\n#         RunnableLambda(\n#             lambda query: parallel_chain.invoke({\n#                 \"books\": query,  # load local book PDFs internally\n#                 \"online_text\": query,                # user query for web search\n#                 \"online_pdf\": query                  # user query for online PDFs\n#             })\n#         ),\n\n#         # Step 3: Combine embeddings and run RAG\n#         combine_chain\n#     ]\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}