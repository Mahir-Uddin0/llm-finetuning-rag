{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13466316,"sourceType":"datasetVersion","datasetId":8548198},{"sourceId":426330,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":347541,"modelId":368803},{"sourceId":426333,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":347543,"modelId":368803},{"sourceId":621762,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":467636,"modelId":483462}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q langchain faiss-cpu unstructured PyPDF2\n!pip install -q huggingface_hub\n!pip install -U langchain-community langchain-huggingface\n!pip install -q langchain-huggingface\n!pip install transformers datasets tqdm ddgs\n!pip install langchain-embedding\n!pip install InstructorEmbedding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:14:12.046230Z","iopub.execute_input":"2025-10-29T14:14:12.046550Z","iopub.status.idle":"2025-10-29T14:15:04.198075Z","shell.execute_reply.started":"2025-10-29T14:14:12.046524Z","shell.execute_reply":"2025-10-29T14:15:04.197178Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 25.1.0 which is incompatible.\ngradio 5.38.1 requires aiofiles<25.0,>=22.0, but you have aiofiles 25.1.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting langchain-community\n  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting langchain-huggingface\n  Downloading langchain_huggingface-1.0.0-py3-none-any.whl.metadata (2.1 kB)\nCollecting langchain-core<2.0.0,>=1.0.1 (from langchain-community)\n  Downloading langchain_core-1.0.1-py3-none-any.whl.metadata (3.5 kB)\nCollecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\nRequirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.5)\nRequirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.3)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\nRequirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.11.0)\nRequirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\nRequirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\nCollecting huggingface-hub<1.0.0,>=0.33.4 (from langchain-huggingface)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.19.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (25.0)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.1.10)\nCollecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.0a1)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.0)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2.4.1)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.8.3)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.37.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.37.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain-community) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\nDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_huggingface-1.0.0-py3-none-any.whl (27 kB)\nDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-1.0.1-py3-none-any.whl (467 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.1/467.1 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\nInstalling collected packages: huggingface-hub, langchain-core, langchain-text-splitters, langchain-huggingface, langchain-classic, langchain-community\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.72\n    Uninstalling langchain-core-0.3.72:\n      Successfully uninstalled langchain-core-0.3.72\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.9\n    Uninstalling langchain-text-splitters-0.3.9:\n      Successfully uninstalled langchain-text-splitters-0.3.9\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlangchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.1 which is incompatible.\nlangchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\ngradio 5.38.1 requires aiofiles<25.0,>=22.0, but you have aiofiles 25.1.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.36.0 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.0.1 langchain-huggingface-1.0.0 langchain-text-splitters-1.0.0\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.1.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nCollecting ddgs\n  Downloading ddgs-9.6.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nCollecting pyarrow>=21.0.0 (from datasets)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from ddgs) (8.3.0)\nCollecting primp>=0.15.0 (from ddgs)\n  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nCollecting lxml>=6.0.0 (from ddgs)\n  Downloading lxml-6.0.2-cp311-cp311-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\nRequirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.28.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (2025.8.3)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.10)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\nRequirement already satisfied: brotli in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.1.0)\nRequirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.3.0)\nCollecting socksio==1.* (from httpx[brotli,http2,socks]>=0.28.1->ddgs)\n  Downloading socksio-1.0.0-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (6.1.0)\nRequirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.1.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.3.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading ddgs-9.6.1-py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading socksio-1.0.0-py3-none-any.whl (12 kB)\nDownloading lxml-6.0.2-cp311-cp311-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: socksio, pyarrow, primp, lxml, ddgs\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n  Attempting uninstall: lxml\n    Found existing installation: lxml 5.4.0\n    Uninstalling lxml-5.4.0:\n      Successfully uninstalled lxml-5.4.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ddgs-9.6.1 lxml-6.0.2 primp-0.15.0 pyarrow-22.0.0 socksio-1.0.0\n\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-embedding (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-embedding\u001b[0m\u001b[31m\n\u001b[0mCollecting InstructorEmbedding\n  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl.metadata (20 kB)\nDownloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\nInstalling collected packages: InstructorEmbedding\nSuccessfully installed InstructorEmbedding-1.0.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nfrom tqdm import tqdm\nfrom langchain.vectorstores import FAISS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:15:04.200200Z","iopub.execute_input":"2025-10-29T14:15:04.200555Z","iopub.status.idle":"2025-10-29T14:15:05.809744Z","shell.execute_reply.started":"2025-10-29T14:15:04.200524Z","shell.execute_reply":"2025-10-29T14:15:05.808995Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Model Load","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom langchain.llms import HuggingFacePipeline\nfrom InstructorEmbedding import INSTRUCTOR\nfrom langchain_community.embeddings import HuggingFaceInstructEmbeddings\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# === 1️⃣ Load fine-tuned financial LLaMA3 model for .invoke() ===\nllm_model_path = \"/kaggle/input/investing-fine-tuned-model-llama-3-2/other/default/1\"\nllm_tokenizer = AutoTokenizer.from_pretrained(llm_model_path)\nllm_model = AutoModelForCausalLM.from_pretrained(llm_model_path, torch_dtype=torch.float16, device_map=\"cpu\")\n\nllm_pipeline = pipeline(\n    \"text-generation\",\n    model=llm_model,\n    tokenizer=llm_tokenizer,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    max_new_tokens=256,\n    temperature=0.2,\n    do_sample=False\n)\nllm = HuggingFacePipeline(pipeline=llm_pipeline)\nprint(\"✅ Fine-tuned financial model loaded.\")\n\n# === 2️⃣ Load local INSTRUCTOR embedding model ===\n# Use 4B parameter model to get better answer. \nembedding_model_path = \"/kaggle/input/qwen-3-embedding/transformers/4b/1\"  # 4B parameters\n# embedding_model_path = \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\"  # 0.6B parameters\n\nembeddings_model = HuggingFaceInstructEmbeddings(\n    model_name=embedding_model_path,\n    model_kwargs={\"device\": device}\n)\n\nprint(\"✅ Embedding model loaded successfully.\")\n\n# LLM\nresponse = llm.invoke(\"Get Tesla 2023 annual report\")\nprint(response)\n\n# Embeddings\nvector = embeddings_model.embed_documents(\"Tesla revenue 2023\")\nprint(len(vector))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:15:05.810560Z","iopub.execute_input":"2025-10-29T14:15:05.811068Z","iopub.status.idle":"2025-10-29T14:19:07.883990Z","shell.execute_reply.started":"2025-10-29T14:15:05.811038Z","shell.execute_reply":"2025-10-29T14:19:07.883014Z"}},"outputs":[{"name":"stderr","text":"2025-10-29 14:15:23.437506: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761747323.736710      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761747323.814235      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca4f8c71ced54fd084d44c86babd32fe"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n/tmp/ipykernel_37/4132251270.py:25: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFacePipeline``.\n  llm = HuggingFacePipeline(pipeline=llm_pipeline)\n/tmp/ipykernel_37/4132251270.py:34: LangChainDeprecationWarning: The class `HuggingFaceInstructEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n  embeddings_model = HuggingFaceInstructEmbeddings(\n","output_type":"stream"},{"name":"stdout","text":"✅ Fine-tuned financial model loaded.\n","output_type":"stream"},{"name":"stderr","text":"`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True}. If this is not desired, please set these values explicitly.\n","output_type":"stream"},{"name":"stdout","text":"✅ Embedding model loaded successfully.\nGet Tesla 2023 annual report\nGet Tesla 2023 annual report. (Note: The report might be available in the form of a PDF or a digital version.) (Also, please note that the availability of the report is subject to change, as it is not explicitly mentioned in the passage.) \n\nThe passage does not provide information about the availability of the Tesla 2023 annual report. It only mentions that it is mentioned. To know the actual availability, you would need to follow the given link or contact the company directly. (Note: The link provided in the passage is not explicitly mentioned to be a direct source for the report's availability.)) \n\nThe passage also mentions that the report might be available in the form of a PDF or a digital version. However, the specific details about the form and format are not provided. To know the exact details, you would need to follow the given link or contact the company directly. (Again, the link provided in the passage is not explicitly mentioned to be a direct source for the report's availability.) \n\nIn summary, the passage does not provide information about the availability of the Tesla 2023 annual report. To know the actual availability, you would need to follow the given link or contact the company directly. The report's form and format details are\n18\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Common Functions","metadata":{}},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.documents import Document\nfrom typing import List\n\ndef get_split_docs(documents):\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=2000,\n        chunk_overlap=350\n    )\n    split_docs = text_splitter.split_documents(documents)\n    print(f\"Created {len(split_docs)} text chunks.\")\n    return split_docs\n\n\n# Example documents\ndocs = [\n    Document(page_content=\"This is the content of document 1.\"),\n    Document(page_content=\"This is the content of document 2.\")\n]\n\nchunks = get_split_docs(docs)\nprint(chunks[0].page_content)  # first chunk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:19:07.886387Z","iopub.execute_input":"2025-10-29T14:19:07.886663Z","iopub.status.idle":"2025-10-29T14:19:12.992414Z","shell.execute_reply.started":"2025-10-29T14:19:07.886643Z","shell.execute_reply":"2025-10-29T14:19:12.991478Z"}},"outputs":[{"name":"stdout","text":"Created 2 text chunks.\nThis is the content of document 1.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from langchain.vectorstores import FAISS\n\ndef get_combined_vector_db(*args):\n    combined_docs = []\n    for doc_list in args:\n        if doc_list:  # make sure it's not None or empty\n            combined_docs.extend(doc_list)\n    \n    # Create vector DB for all docs\n    combined_vector_db = FAISS.from_documents(combined_docs, embeddings_model)\n    \n    return combined_vector_db\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:19:12.993355Z","iopub.execute_input":"2025-10-29T14:19:12.994141Z","iopub.status.idle":"2025-10-29T14:19:12.999064Z","shell.execute_reply.started":"2025-10-29T14:19:12.994119Z","shell.execute_reply":"2025-10-29T14:19:12.998161Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from langchain_community.document_loaders import PyPDFLoader\n\ndef get_pdf_split_docs(pdf_folder):\n    pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n    all_documents = []\n    \n    for pdf_path in pdf_files:\n        loader = PyPDFLoader(pdf_path)\n        docs = loader.load()  # this returns list of Document objects\n        all_documents.extend(docs)\n        \n    split_docs = get_split_docs(all_documents)\n    \n    return split_docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:19:13.000053Z","iopub.execute_input":"2025-10-29T14:19:13.000399Z","iopub.status.idle":"2025-10-29T14:19:13.062550Z","shell.execute_reply.started":"2025-10-29T14:19:13.000371Z","shell.execute_reply":"2025-10-29T14:19:13.061714Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Three sources of information will be used. \n### 1. PDF books added to Kaggle input directory\n### 2. Web search results\n### 3. Company Annual Reports","metadata":{}},{"cell_type":"markdown","source":"# 1. PDF Embedding","metadata":{}},{"cell_type":"code","source":"pdf_folder = \"/kaggle/input/investing-books-pdf\"\nlocal_pdf_split_docs = get_pdf_split_docs(pdf_folder)\n\ndef get_local_pdf_split_docs():\n    return local_pdf_split_docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:19:13.063391Z","iopub.execute_input":"2025-10-29T14:19:13.063717Z","iopub.status.idle":"2025-10-29T14:20:49.460852Z","shell.execute_reply.started":"2025-10-29T14:19:13.063686Z","shell.execute_reply":"2025-10-29T14:20:49.460102Z"}},"outputs":[{"name":"stdout","text":"Created 8126 text chunks.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# 2. Web Search","metadata":{}},{"cell_type":"code","source":"from ddgs import DDGS\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef web_search(query, max_results=3):\n    docs = []\n\n    # Search the web using DuckDuckGo\n    with DDGS() as ddgs:\n        results = list(ddgs.text(query, max_results=max_results))\n\n    for result in results:\n        url = result.get(\"href\")\n        title = result.get(\"title\")\n\n        try:\n            # Fetch the web page content\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n\n            # Parse visible text from HTML\n            soup = BeautifulSoup(response.text, \"html.parser\")\n            for tag in soup([\"script\", \"style\", \"noscript\"]):\n                tag.extract()\n            text = \" \".join(soup.get_text(separator=\" \").split())\n\n            # Only keep if text length is reasonable\n            if len(text) > 500:\n                docs.append(Document(page_content=text, metadata={\"source\": url, \"title\": title}))\n        except Exception as e:\n            pass\n\n    # print(f\"✅ Retrieved {len(docs)} web documents from top {max_results} results.\")\n    return docs\n\n# query = \"Tesla 2025 annual revenue analysis\"\n# documents = web_search(query)\n\n# print(f\"Number of documents: {len(documents)}\")\n# print(documents[0].metadata)\n# print(documents[0].page_content[:500])  # preview text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:20:49.462444Z","iopub.execute_input":"2025-10-29T14:20:49.463217Z","iopub.status.idle":"2025-10-29T14:20:49.504020Z","shell.execute_reply.started":"2025-10-29T14:20:49.463192Z","shell.execute_reply":"2025-10-29T14:20:49.503075Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def get_web_search_split_docs(query, max_results=3):\n    docs = web_search(query, max_results)\n    split_docs = get_split_docs(docs)\n    return split_docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:20:49.505269Z","iopub.execute_input":"2025-10-29T14:20:49.505590Z","iopub.status.idle":"2025-10-29T14:20:49.510390Z","shell.execute_reply.started":"2025-10-29T14:20:49.505569Z","shell.execute_reply":"2025-10-29T14:20:49.509494Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# 3. Web PDF Retriever","metadata":{}},{"cell_type":"code","source":"import torch\nimport json\n\ndef analyze_query_with_llm(query: str):\n    \"\"\"\n    Use the fine-tuned LLM to decide if annual report retrieval is required,\n    and extract the company name and year if applicable.\n\n    Returns a dictionary like:\n    {\n        \"required\": True,\n        \"company\": \"Tesla\",\n        \"year\": \"2024\"\n    }\n    \"\"\"\n    prompt = f\"\"\"\n    You are an intelligent financial assistant. \n    Your task is to analyze the user query and decide:\n    1. Whether it requires downloading a company's annual report.\n    2. If yes, extract the company name and the year of the report.\n    \n    Respond strictly in this JSON format:\n    {{\n      \"required\": true or false,\n      \"company\": \"Company name or null\",\n      \"year\": \"Year or null\"\n    }}\n    \n    User query: \"{query}\"\n    \"\"\"\n\n    response_text = llm.invoke(prompt)\n    \n    if \"Answer:\" in response_text:\n        response_text = response_text.split(\"Answer:\")[-1]\n\n    # Try to extract JSON response from model output\n    try:\n        json_start = response_text.find(\"{\")\n        json_end = response_text.rfind(\"}\") + 1\n        json_str = response_text[json_start:json_end]\n        result = json.loads(json_str)\n    except Exception as e:\n        print(\"⚠️ Could not parse model output properly. Raw output:\")\n        # print(response_text)\n        result = {\"required\": False, \"company\": None, \"year\": None}\n\n    return result\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:20:49.513059Z","iopub.execute_input":"2025-10-29T14:20:49.513365Z","iopub.status.idle":"2025-10-29T14:20:49.531809Z","shell.execute_reply.started":"2025-10-29T14:20:49.513344Z","shell.execute_reply":"2025-10-29T14:20:49.530771Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from ddgs import DDGS\n\ndef search_pdfs(company_name, year=2024, max_results=5):\n    query = f\"{company_name} {year} annual report filetype:pdf\"\n    with DDGS() as ddgs:\n        results = list(ddgs.text(query, max_results=max_results))\n    pdf_links = [r['href'] for r in results if r['href'].endswith('.pdf')]\n    return pdf_links\n\n# Example:\npdf_urls = search_pdfs(\"Square Pharma\", 2024)\nprint(pdf_urls)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:20:49.532697Z","iopub.execute_input":"2025-10-29T14:20:49.532935Z","iopub.status.idle":"2025-10-29T14:20:50.893973Z","shell.execute_reply.started":"2025-10-29T14:20:49.532914Z","shell.execute_reply":"2025-10-29T14:20:50.893068Z"}},"outputs":[{"name":"stdout","text":"['https://www.squarepharma.com.bd/downloads/Square+Pharma_AR_24+dt-24-11-24_compressed_1.pdf', 'https://web.hd.square-enix.com/eng/ir/library/pdf/ar_2024en.pdf', 'https://www.squarepharma.com.bd/SPL+1st+Qtr+Financial+Report+2023-2024.pdf', 'https://www.squarepharma.com.bd/Latest+Audited+Financial+Statement.pdf', 'https://www.squarepharma.com.bd/1st+QTR+SPL+2024-25+PDF_1.pdf']\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import requests\n\ndef download_pdf(pdf_links, save_dir):\n    os.makedirs(save_dir, exist_ok=True)  # ensure directory exists\n\n    for i, url in enumerate(pdf_links, 1):\n        response = requests.get(url, stream=True)\n        if response.status_code == 200 and 'application/pdf' in response.headers.get('Content-Type', ''):\n            file_path = os.path.join(save_dir, f\"web_pdf_{i}.pdf\")\n            with open(file_path, 'wb') as f:\n                for chunk in response.iter_content(1024):\n                    f.write(chunk)\n            print(f\"✅ Downloaded: {file_path}\")\n        else:\n            print(f\"❌ Failed to download from {url}\")\n\n# for i in range(len(pdf_urls)):\n# download_pdf(pdf_urls[4], \"apple_2024_annual_report.pdf\")\n# download_pdf(\"https://www.squarepharma.com.bd/downloads/Square%20Pharma_AR_24%20dt-24-11-24_compressed_1.pdf\", \"hjd.pdf\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:20:50.894879Z","iopub.execute_input":"2025-10-29T14:20:50.895144Z","iopub.status.idle":"2025-10-29T14:20:50.901769Z","shell.execute_reply.started":"2025-10-29T14:20:50.895125Z","shell.execute_reply":"2025-10-29T14:20:50.900724Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import shutil\ndef clean_output_dir():\n    path = \"/kaggle/working/\"\n    for f in os.listdir(path):\n        fp = os.path.join(path, f)\n        if os.path.isfile(fp) or os.path.islink(fp):\n            os.unlink(fp)\n        elif os.path.isdir(fp):\n            shutil.rmtree(fp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:20:50.902690Z","iopub.execute_input":"2025-10-29T14:20:50.902987Z","iopub.status.idle":"2025-10-29T14:20:50.917937Z","shell.execute_reply.started":"2025-10-29T14:20:50.902960Z","shell.execute_reply":"2025-10-29T14:20:50.917186Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def get_web_pdf_split_docs(query, max_results=5):\n    result = analyze_query_with_llm(query) \n    \n    pdf_links = search_pdfs(result['company'], result['year'])\n    \n    output_dir = \"/kaggle/working/\"\n    \n    download_pdf(pdf_links, output_dir)\n    \n    split_docs = get_pdf_split_docs(output_dir)\n\n    return split_docs\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:20:50.918921Z","iopub.execute_input":"2025-10-29T14:20:50.919265Z","iopub.status.idle":"2025-10-29T14:20:50.935749Z","shell.execute_reply.started":"2025-10-29T14:20:50.919232Z","shell.execute_reply":"2025-10-29T14:20:50.934894Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Retrieval","metadata":{}},{"cell_type":"code","source":"# def rag_retrieval(vector_db, query):\n#     \"\"\"\n#     Perform similarity search on the vector DB using the query.\n#     Returns top relevant documents.\n#     \"\"\"\n#     return vector_db.similarity_search(query, k=5)  # top 5 results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:20:50.936821Z","iopub.execute_input":"2025-10-29T14:20:50.937151Z","iopub.status.idle":"2025-10-29T14:20:50.952398Z","shell.execute_reply.started":"2025-10-29T14:20:50.937127Z","shell.execute_reply":"2025-10-29T14:20:50.951434Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Augmentation","metadata":{}},{"cell_type":"code","source":"from langchain_core.prompts import PromptTemplate # <-- Fix this line\n\n# Define the template string\nprompt = \"\"\"\nYou are a professional financial and investing expert specializing in long-term investing, company analysis, stock market strategies, and business insights. \nYou have deep knowledge of business fundamentals, financial statements, market trends, competition, consumer behavior, and investment analysis.\n\nAnswer the user question using ONLY the provided context below. \nDo not hallucinate, do not make assumptions beyond the context, and do not answer questions outside the domain of finance, investing, business, entrepreneurship, competition, marketing, or consumer psychology. \nIf the context is insufficient or the question is outside your domain, respond with: \"Insufficient data provided.\"\n\nContext:\n{context}\n\nUser Question:\n{question}\n\nAnswer:\n\"\"\"\n\n# Create a PromptTemplate instance\ninvesting_prompt = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=prompt\n)\n\n# Example usage\n# retrieved_context = \"Tesla's revenue in 2023 increased by 15% compared to 2022. Gross margin is 25%.\"\n# user_query = \"Should I invest in Tesla for the next 5 years?\"\n\n# final_prompt = investing_prompt.format(context=retrieved_context, question=user_query)\n# print(final_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:20:50.953290Z","iopub.execute_input":"2025-10-29T14:20:50.953518Z","iopub.status.idle":"2025-10-29T14:20:50.989738Z","shell.execute_reply.started":"2025-10-29T14:20:50.953501Z","shell.execute_reply":"2025-10-29T14:20:50.989004Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Generation","metadata":{}},{"cell_type":"markdown","source":"### After user query, three functions are called to get split docs from three different data source. Then, the split docs are combined and embedded with the get_combined_vector_db function. All,the embedded vectors are stored in one vector_db to retrieve relevant context for prompt augmentation.","metadata":{}},{"cell_type":"code","source":"def get_vector_db(query):\n    clean_output_dir() \n    \n    local_pdf_split_docs = get_local_pdf_split_docs()    \n    web_pdf_split_docs = get_web_pdf_split_docs(query)\n    web_search_split_docs = get_web_pdf_split_docs(query)\n    \n    vector_db = get_combined_vector_db(local_pdf_split_docs, web_pdf_split_docs, web_search_split_docs)\n    \n    return vector_db","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:20:50.990740Z","iopub.execute_input":"2025-10-29T14:20:50.991032Z","iopub.status.idle":"2025-10-29T14:20:50.995668Z","shell.execute_reply.started":"2025-10-29T14:20:50.991009Z","shell.execute_reply":"2025-10-29T14:20:50.994878Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def generate_ans(query):\n    vector_db = get_vector_db(query)    # Get Vector Database\n    \n    retrieved_context = vector_db.similarity_search(query, k=5)     # Retrieve\n\n    retrieved_context = context()\n    \n    final_prompt = investing_prompt.format(context=retrieved_context, question=query)   # Augmentation\n    \n    ans = llm.invoke(final_prompt)    # Generation\n    if \"Answer:\" in ans:\n        ans = ans.split(\"Answer:\")[-1]\n    \n    print(ans)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:20:50.996647Z","iopub.execute_input":"2025-10-29T14:20:50.996917Z","iopub.status.idle":"2025-10-29T14:20:51.015451Z","shell.execute_reply.started":"2025-10-29T14:20:50.996889Z","shell.execute_reply":"2025-10-29T14:20:51.014259Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"Give you query in the below cell, and run it.","metadata":{}},{"cell_type":"code","source":"query = \"Give me detailed fundamental analysis on Marico Bangladesh Limited\"\ngenerate_ans(query)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:20:51.016476Z","iopub.execute_input":"2025-10-29T14:20:51.016703Z","iopub.status.idle":"2025-10-29T14:20:51.036616Z","shell.execute_reply.started":"2025-10-29T14:20:51.016685Z","shell.execute_reply":"2025-10-29T14:20:51.035730Z"}},"outputs":[{"name":"stdout","text":"Marico Bangladesh Limited is a consumer-products company operating in Bangladesh and is a subsidiary of India-based Marico Limited. The company produces coconut oil under the Parachute brand, hair oils such as Nihar Naturals, and other personal care products. Its products are distributed through sales depots in multiple regions, and it has a recognizable brand presence in the country. In FY 2024, the company reported revenue growth of about 12% compared to FY 2023, with net profit increasing by roughly 28%. Its balance sheet shows low debt and strong equity, indicating a stable financial position. The company maintains a diversified product mix, which helps reduce reliance on a single category. Some risks include input cost pressures and potential market growth moderation. Overall, Marico Bangladesh appears reasonably positioned for long-term operations, but investors should monitor cost control, competition, and market trends before making investment decisions.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"Generating answer with CPU takes 8 hour+ because of embedding huge documents with a 4B parameter size model. Using GPU, the session will break because of exhausting 16 GB GPU memory. The code will break saying, \"CUDA out of memory. Tried to allocate 420.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 147.12 MiB is free. Process 3317 has 15.74 GiB memory in use.\"\n\nChat option is not added yet. The model will not remember previous conversations.","metadata":{}},{"cell_type":"markdown","source":"# Chain Construction","metadata":{}},{"cell_type":"code","source":"# from langchain.schema import Document\n# from langchain.runnables import (\n#     RunnablePassthrough,\n#     RunnableLambda,\n#     RunnableSequence,\n#     RunnableParallel,\n# )\n\n# # 2️⃣ Parallel execution of three branches\n# parallel_chain = RunnableParallel(\n#     {\n#         \"books\": RunnableSequence(\n#             [\n#                 RunnableLambda(lambda query: get_local_pdf_split_docs(query)),   # split preloaded book PDFs\n#             ]\n#         ),\n#         \"online_text\": RunnableSequence(\n#             [\n#                 RunnableLambda(lambda query: get_web_search_split_docs(query)),  # search + split text\n#             ]\n#         ),\n#         \"online_pdf\": RunnableSequence(\n#             [\n#                 RunnableLambda(lambda query: get_web_pdf_split_docs(query)),  # download + split PDFs\n#             ]\n#         ),\n#     },\n#     combine_mode=\"list\",  # returns a list of outputs for each branch\n# )\n\n# # 3️⃣ Combine embeddings and build RAG pipeline\n# combine_chain = RunnableSequence(\n#     [\n#         RunnableLambda(lambda split_docs_lists: get_combined_vector_db(*split_docs_lists)),  # combine all split docs\n#         RunnableLambda(lambda vector_db: rag_retrieval(vector_db)),                           # retrieval + augmentation\n#         RunnableLambda(lambda result: generate_answer(result)),                               # LLM generation\n#     ]\n# )\n\n# # 4️⃣ Full pipeline: sequence of parallel + combine\n# full_pipeline = RunnableSequence(\n#     [\n#         RunnablePassthrough(),\n\n#         # Step 2: Parallel execution\n#         RunnableLambda(\n#             lambda query: parallel_chain.invoke({\n#                 \"books\": query,  # load local book PDFs internally\n#                 \"online_text\": query,                # user query for web search\n#                 \"online_pdf\": query                  # user query for online PDFs\n#             })\n#         ),\n\n#         # Step 3: Combine embeddings and run RAG\n#         combine_chain\n#     ]\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:20:51.037786Z","iopub.execute_input":"2025-10-29T14:20:51.038440Z","iopub.status.idle":"2025-10-29T14:20:51.054881Z","shell.execute_reply.started":"2025-10-29T14:20:51.038409Z","shell.execute_reply":"2025-10-29T14:20:51.053695Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}