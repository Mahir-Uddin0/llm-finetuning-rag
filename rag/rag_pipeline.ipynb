{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13466316,"sourceType":"datasetVersion","datasetId":8548198},{"sourceId":426333,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":347543,"modelId":368803},{"sourceId":621762,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":467636,"modelId":483462}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q langchain faiss-cpu unstructured PyPDF2\n!pip install -q huggingface_hub\n!pip install -U langchain-community langchain-huggingface\n!pip install -q langchain-huggingface\n!pip install transformers datasets tqdm ddgs\n!pip install langchain-embedding\n!pip install InstructorEmbedding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:43:10.415658Z","iopub.execute_input":"2025-10-28T16:43:10.416304Z","iopub.status.idle":"2025-10-28T16:43:36.102886Z","shell.execute_reply.started":"2025-10-28T16:43:10.416274Z","shell.execute_reply":"2025-10-28T16:43:36.101915Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.8/449.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain-classic 1.0.0 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.3.79 which is incompatible.\nlangchain-classic 1.0.0 requires langchain-text-splitters<2.0.0,>=1.0.0, but you have langchain-text-splitters 0.3.11 which is incompatible.\nlangchain-huggingface 1.0.0 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.3.79 which is incompatible.\nlangchain-community 0.4.1 requires langchain-core<2.0.0,>=1.0.1, but you have langchain-core 0.3.79 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.4.1)\nRequirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (1.0.0)\nCollecting langchain-core<2.0.0,>=1.0.1 (from langchain-community)\n  Using cached langchain_core-1.0.1-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.0.0)\nRequirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\nRequirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.5)\nRequirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.3)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\nRequirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.11.0)\nRequirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\nRequirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.36.0)\nRequirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.19.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (25.0)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.1.10)\nCollecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n  Using cached langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.0a1)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.0)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2.4.1)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.8.3)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.37.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.37.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain-community) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\nUsing cached langchain_core-1.0.1-py3-none-any.whl (467 kB)\nUsing cached langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\nInstalling collected packages: langchain-core, langchain-text-splitters\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.79\n    Uninstalling langchain-core-0.3.79:\n      Successfully uninstalled langchain-core-0.3.79\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.11\n    Uninstalling langchain-text-splitters-0.3.11:\n      Successfully uninstalled langchain-text-splitters-0.3.11\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.1 which is incompatible.\nlangchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-core-1.0.1 langchain-text-splitters-1.0.0\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.1.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: ddgs in /usr/local/lib/python3.11/dist-packages (9.6.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from ddgs) (8.3.0)\nRequirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from ddgs) (0.15.0)\nRequirement already satisfied: lxml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from ddgs) (6.0.2)\nRequirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.28.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (2025.8.3)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.10)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\nRequirement already satisfied: brotli in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.1.0)\nRequirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.3.0)\nRequirement already satisfied: socksio==1.* in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (6.1.0)\nRequirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.1.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.3.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-embedding (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-embedding\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: InstructorEmbedding in /usr/local/lib/python3.11/dist-packages (1.0.1)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nfrom tqdm import tqdm\nfrom langchain.vectorstores import FAISS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:43:36.104817Z","iopub.execute_input":"2025-10-28T16:43:36.105098Z","iopub.status.idle":"2025-10-28T16:43:37.081923Z","shell.execute_reply.started":"2025-10-28T16:43:36.105062Z","shell.execute_reply":"2025-10-28T16:43:37.081185Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Model Load","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom langchain.llms import HuggingFacePipeline\nfrom InstructorEmbedding import INSTRUCTOR\nfrom langchain_community.embeddings import HuggingFaceInstructEmbeddings\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# === 1️⃣ Load fine-tuned financial LLaMA3 model for .invoke() ===\nllm_model_path = \"/kaggle/input/investing-fine-tuned-model-llama-3-2/other/default/1\"\nllm_tokenizer = AutoTokenizer.from_pretrained(llm_model_path)\nllm_model = AutoModelForCausalLM.from_pretrained(llm_model_path, torch_dtype=torch.float16, device_map=\"cpu\")\n\nllm_pipeline = pipeline(\n    \"text-generation\",\n    model=llm_model,\n    tokenizer=llm_tokenizer,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    max_new_tokens=256,\n    temperature=0.2,\n    do_sample=False\n)\nllm = HuggingFacePipeline(pipeline=llm_pipeline)\nprint(\"✅ Fine-tuned financial model loaded.\")\n\n# === 2️⃣ Load local INSTRUCTOR embedding model ===\n# embedding_model_path = \"/kaggle/input/qwen-3-embedding/transformers/4b/1\"  # 4B parameters\nembedding_model_path = \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\"  # 0.6B parameters\n# embeddings_model = INSTRUCTOR(embedding_model_path, device=device)\n\nembeddings_model = HuggingFaceInstructEmbeddings(\n    model_name=embedding_model_path,\n    model_kwargs={\"device\": device}\n)\n\nprint(\"✅ Embedding model loaded successfully.\")\n\n# LLM\nresponse = llm.invoke(\"Get Tesla 2023 annual report\")\nprint(response)\n\n# Embeddings\nvector = embeddings_model.embed_documents(\"Tesla revenue 2023\")\nprint(len(vector))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:48:04.173142Z","iopub.execute_input":"2025-10-28T16:48:04.173956Z","iopub.status.idle":"2025-10-28T16:48:20.982723Z","shell.execute_reply.started":"2025-10-28T16:48:04.173920Z","shell.execute_reply":"2025-10-28T16:48:20.981902Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGet Tesla 2023 annual report\nThe passage is referring to the Tesla 2023 annual report. However, it does not provide any details about the content of the report. To know the details, you would need to refer to the report itself. Additionally, the passage does not provide any context or information about the report's release date or author. For a complete understanding, you would need to refer to the report itself. (Note: The passage is not providing the actual details, but it mentions the report itself, which might be a reference to a different document or event.) (Note: The passage is not providing the actual details, but it mentions the report itself, which might be a reference to a different document or event.) (Note: The passage is not providing the actual details, but it mentions the report itself, which might be a reference to a different document or event.) (Note: The passage is not providing the actual details, but it mentions the report itself, which might be a reference to a different document or event.) (Note: The passage is not providing the actual details, but it mentions the report itself, which might be a reference to a different document or event.) (Note: The passage is not providing the actual details, but it mentions the report itself, which might be\n18\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Common Functions","metadata":{}},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.documents import Document\nfrom typing import List\n\ndef get_split_docs(documents):\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=2000,\n        chunk_overlap=350\n    )\n    split_docs = text_splitter.split_documents(documents)\n    print(f\"Created {len(split_docs)} text chunks.\")\n    return split_docs\n\n\n# Example documents\ndocs = [\n    Document(page_content=\"This is the content of document 1.\"),\n    Document(page_content=\"This is the content of document 2.\")\n]\n\nchunks = get_split_docs(docs)\nprint(chunks[0].page_content)  # first chunk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:53:42.343776Z","iopub.execute_input":"2025-10-28T16:53:42.344486Z","iopub.status.idle":"2025-10-28T16:53:45.236673Z","shell.execute_reply.started":"2025-10-28T16:53:42.344456Z","shell.execute_reply":"2025-10-28T16:53:45.235764Z"}},"outputs":[{"name":"stdout","text":"Created 2 text chunks.\nThis is the content of document 1.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def get_combined_vector_db(*args):\n    combined_docs = []\n    for doc_list in args:\n        if doc_list:  # make sure it's not None or empty\n            combined_docs.extend(doc_list)\n    \n    # Create vector DB for all docs\n    combined_vector_db = FAISS.from_documents(combined_docs, embeddings_model)\n    \n    return combined_vector_db\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:53:45.238447Z","iopub.execute_input":"2025-10-28T16:53:45.239147Z","iopub.status.idle":"2025-10-28T16:53:45.243635Z","shell.execute_reply.started":"2025-10-28T16:53:45.239124Z","shell.execute_reply":"2025-10-28T16:53:45.242848Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from langchain_community.document_loaders import PyPDFLoader\ndef get_pdf_split_docs(pdf_folder):\n    pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n    all_documents = []\n    \n    for pdf_path in pdf_files:\n        loader = PyPDFLoader(pdf_path)\n        docs = loader.load()  # this returns list of Document objects\n        all_documents.extend(docs)\n        \n    split_docs = get_split_docs(all_documents)\n    \n    return split_docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:53:45.244456Z","iopub.execute_input":"2025-10-28T16:53:45.244695Z","iopub.status.idle":"2025-10-28T16:53:45.303846Z","shell.execute_reply.started":"2025-10-28T16:53:45.244677Z","shell.execute_reply":"2025-10-28T16:53:45.303339Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Three sources of information will be used. \n### 1. PDF books added to Kaggle input directory\n### 2. Web search results\n### 3. Company Annual Reports","metadata":{}},{"cell_type":"markdown","source":"# 1. PDF Embedding","metadata":{}},{"cell_type":"code","source":"pdf_folder = \"/kaggle/input/investing-books-pdf\"\nlocal_pdf_split_docs = get_pdf_split_docs(pdf_folder)\n\ndef get_local_pdf_split_docs():\n    return local_pdf_split_docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:53:45.304503Z","iopub.execute_input":"2025-10-28T16:53:45.304753Z","iopub.status.idle":"2025-10-28T16:55:04.946979Z","shell.execute_reply.started":"2025-10-28T16:53:45.304737Z","shell.execute_reply":"2025-10-28T16:55:04.946122Z"}},"outputs":[{"name":"stdout","text":"Created 8126 text chunks.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# 2. Web Search","metadata":{}},{"cell_type":"code","source":"from ddgs import DDGS\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef web_search(query, max_results=3):\n    docs = []\n\n    # Search the web using DuckDuckGo\n    with DDGS() as ddgs:\n        results = list(ddgs.text(query, max_results=max_results))\n\n    for result in results:\n        url = result.get(\"href\")\n        title = result.get(\"title\")\n\n        try:\n            # Fetch the web page content\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n\n            # Parse visible text from HTML\n            soup = BeautifulSoup(response.text, \"html.parser\")\n            for tag in soup([\"script\", \"style\", \"noscript\"]):\n                tag.extract()\n            text = \" \".join(soup.get_text(separator=\" \").split())\n\n            # Only keep if text length is reasonable\n            if len(text) > 500:\n                docs.append(Document(page_content=text, metadata={\"source\": url, \"title\": title}))\n        except Exception as e:\n            pass\n\n    # print(f\"✅ Retrieved {len(docs)} web documents from top {max_results} results.\")\n    return docs\n\nquery = \"Tesla 2025 annual revenue analysis\"\ndocuments = web_search(query)\n\nprint(f\"Number of documents: {len(documents)}\")\nprint(documents[0].metadata)\nprint(documents[0].page_content[:500])  # preview text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:55:04.949553Z","iopub.execute_input":"2025-10-28T16:55:04.950266Z","iopub.status.idle":"2025-10-28T16:55:10.061699Z","shell.execute_reply.started":"2025-10-28T16:55:04.950246Z","shell.execute_reply":"2025-10-28T16:55:10.060795Z"}},"outputs":[{"name":"stdout","text":"Number of documents: 1\n{'source': 'https://carbuzz.com/12-percent-tesla-owners-choose-full-self-driving/', 'title': 'Tesla Customers Are Not Paying Extra To Get FSD'}\nTesla Customers Are Not Paying Extra To Get FSD Menu Sign in now Close News Features Car Brands Best Cars Submenu Best SUVs Best Crossovers Best Trucks Best Vans Best Sedans Best Coupes Best Hatchbacks Best Convertibles Best Hybrid Cars Best Electric Cars Best Sports Cars Best Luxury Cars Best Small Cars Best Wagons Car Comparisons Reviews Car Advice Videos Threads CarBuzz Awards Sign in Newsletter Menu Follow Followed Like Threads More Action Summary Generate a summary of this story Sign in now\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def get_web_search_split_docs(query, max_results=3):\n    docs = web_search(query, max_results)\n    split_docs = get_split_docs(docs)\n    return split_docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:55:10.062887Z","iopub.execute_input":"2025-10-28T16:55:10.063222Z","iopub.status.idle":"2025-10-28T16:55:10.067340Z","shell.execute_reply.started":"2025-10-28T16:55:10.063200Z","shell.execute_reply":"2025-10-28T16:55:10.066533Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# 3. Web PDF Retriever","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nimport json\n\ndef analyze_query_with_llm(query: str):\n    \"\"\"\n    Use the fine-tuned LLM to decide if annual report retrieval is required,\n    and extract the company name and year if applicable.\n\n    Returns a dictionary like:\n    {\n        \"required\": True,\n        \"company\": \"Tesla\",\n        \"year\": \"2024\"\n    }\n    \"\"\"\n    prompt = f\"\"\"\n    You are an intelligent financial assistant. \n    Your task is to analyze the user query and decide:\n    1. Whether it requires downloading a company's annual report.\n    2. If yes, extract the company name and the year of the report.\n    \n    Respond strictly in this JSON format:\n    {{\n      \"required\": true or false,\n      \"company\": \"Company name or null\",\n      \"year\": \"Year or null\"\n    }}\n    \n    User query: \"{query}\"\n    \"\"\"\n    # Tokenize input\n    inputs = llm_tokenizer(prompt, return_tensors=\"pt\").to(llm_model.device)\n\n    # Generate model output\n    outputs = llm_model.generate(\n        **inputs,\n        max_new_tokens=256,\n        temperature=0.2,\n        do_sample=False\n    )\n\n    response_text = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Try to extract JSON response from model output\n    try:\n        json_start = response_text.find(\"{\")\n        json_end = response_text.rfind(\"}\") + 1\n        json_str = response_text[json_start:json_end]\n        result = json.loads(json_str)\n    except Exception as e:\n        print(\"⚠️ Could not parse model output properly. Raw output:\")\n        # print(response_text)\n        result = {\"required\": False, \"company\": None, \"year\": None}\n\n    return result\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:55:10.068264Z","iopub.execute_input":"2025-10-28T16:55:10.068506Z","iopub.status.idle":"2025-10-28T16:55:10.079811Z","shell.execute_reply.started":"2025-10-28T16:55:10.068486Z","shell.execute_reply":"2025-10-28T16:55:10.079116Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from ddgs import DDGS\n\ndef search_pdfs(company_name, year=2024, max_results=5):\n    query = f\"{company_name} {year} annual report filetype:pdf\"\n    with DDGS() as ddgs:\n        results = list(ddgs.text(query, max_results=max_results))\n    pdf_links = [r['href'] for r in results if r['href'].endswith('.pdf')]\n    return pdf_links\n\n# Example:\npdf_urls = search_pdfs(\"Square Pharma\", 2024)\nprint(pdf_urls)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:55:10.080621Z","iopub.execute_input":"2025-10-28T16:55:10.081204Z","iopub.status.idle":"2025-10-28T16:55:12.348985Z","shell.execute_reply.started":"2025-10-28T16:55:10.081184Z","shell.execute_reply":"2025-10-28T16:55:12.348196Z"}},"outputs":[{"name":"stdout","text":"['https://www.squarepharma.com.bd/downloads/Square+Pharma_AR_24+dt-24-11-24_compressed_1.pdf', 'https://pharmamar.com/wp-content/uploads/2025/04/ANNUAL-REPORT-2024.pdf', 'https://www.squarepharma.com.bd/SPL+1st+Qtr+Financial+Report+2023-2024.pdf', 'https://www.squarepharma.com.bd/Latest+Audited+Financial+Statement.pdf', 'https://www.squarepharma.com.bd/Square+Pharma_AR_2023.pdf']\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import requests\n\ndef download_pdf(pdf_links, save_dir):\n    os.makedirs(save_dir, exist_ok=True)  # ensure directory exists\n\n    for i, url in enumerate(pdf_links, 1):\n        response = requests.get(url, stream=True)\n        if response.status_code == 200 and 'application/pdf' in response.headers.get('Content-Type', ''):\n            file_path = os.path.join(save_dir, f\"web_pdf_{i}.pdf\")\n            with open(file_path, 'wb') as f:\n                for chunk in response.iter_content(1024):\n                    f.write(chunk)\n            print(f\"✅ Downloaded: {file_path}\")\n        else:\n            print(f\"❌ Failed to download from {url}\")\n\n# for i in range(len(pdf_urls)):\n# download_pdf(pdf_urls[4], \"apple_2024_annual_report.pdf\")\n# download_pdf(\"https://www.squarepharma.com.bd/downloads/Square%20Pharma_AR_24%20dt-24-11-24_compressed_1.pdf\", \"hjd.pdf\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:55:12.350240Z","iopub.execute_input":"2025-10-28T16:55:12.350533Z","iopub.status.idle":"2025-10-28T16:55:12.356038Z","shell.execute_reply.started":"2025-10-28T16:55:12.350514Z","shell.execute_reply":"2025-10-28T16:55:12.355352Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import shutil\ndef clean_output_dir():\n    path = \"/kaggle/working/\"\n    for f in os.listdir(path):\n        fp = os.path.join(path, f)\n        if os.path.isfile(fp) or os.path.islink(fp):\n            os.unlink(fp)\n        elif os.path.isdir(fp):\n            shutil.rmtree(fp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:55:12.356820Z","iopub.execute_input":"2025-10-28T16:55:12.357364Z","iopub.status.idle":"2025-10-28T16:55:12.367799Z","shell.execute_reply.started":"2025-10-28T16:55:12.357339Z","shell.execute_reply":"2025-10-28T16:55:12.367100Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def get_web_pdf_split_docs(query, max_results=5):\n    result = analyze_query_with_llm(query) \n    \n    pdf_links = search_pdfs(result['company'], result['year'])\n    \n    output_dir = \"/kaggle/working/\"\n    \n    download_pdf(pdf_links, output_dir)\n    \n    split_docs = get_pdf_split_docs(output_dir)\n\n    return split_docs\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:55:12.368462Z","iopub.execute_input":"2025-10-28T16:55:12.368691Z","iopub.status.idle":"2025-10-28T16:55:12.380251Z","shell.execute_reply.started":"2025-10-28T16:55:12.368674Z","shell.execute_reply":"2025-10-28T16:55:12.379468Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Retrieval","metadata":{}},{"cell_type":"code","source":"def rag_retrieval(vector_db, query):\n    \"\"\"\n    Perform similarity search on the vector DB using the query.\n    Returns top relevant documents.\n    \"\"\"\n    return vector_db.similarity_search(query, k=5)  # top 5 results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:55:12.380939Z","iopub.execute_input":"2025-10-28T16:55:12.381572Z","iopub.status.idle":"2025-10-28T16:55:12.391207Z","shell.execute_reply.started":"2025-10-28T16:55:12.381553Z","shell.execute_reply":"2025-10-28T16:55:12.390441Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Augmentation","metadata":{}},{"cell_type":"code","source":"from langchain_core.prompts import PromptTemplate # <-- Fix this line\n\n# Define the template string\nprompt = \"\"\"\nYou are a financial and investing expert specializing in long-term investing, company analysis, and stock market strategies. \nYou have deep knowledge of business fundamentals, financial statements, market trends, and investment analysis.\n\nUse the following context from company reports, web data, and relevant documents to answer the user query. \nAlways base your answers on the provided context and do not make unsupported claims. \n\nContext:\n{context}\n\nUser Question:\n{question}\n\nInstructions:\n- Analyze the financial and business information carefully.\n- Provide long-term investment insights.\n- Give clear reasoning and avoid generic statements.\n- If the context does not provide sufficient information, say \"Insufficient data provided.\"\n- Summarize your analysis in a professional and concise manner.\n\nAnswer:\n\"\"\"\n\n# Create a PromptTemplate instance\ninvesting_prompt = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=prompt\n)\n\n# Example usage\nretrieved_context = \"Tesla's revenue in 2023 increased by 15% compared to 2022. Gross margin is 25%.\"\nuser_query = \"Should I invest in Tesla for the next 5 years?\"\n\nfinal_prompt = investing_prompt.format(context=retrieved_context, question=user_query)\nprint(final_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:55:12.391890Z","iopub.execute_input":"2025-10-28T16:55:12.392225Z","iopub.status.idle":"2025-10-28T16:55:12.422950Z","shell.execute_reply.started":"2025-10-28T16:55:12.392202Z","shell.execute_reply":"2025-10-28T16:55:12.422104Z"}},"outputs":[{"name":"stdout","text":"\nYou are a financial and investing expert specializing in long-term investing, company analysis, and stock market strategies. \nYou have deep knowledge of business fundamentals, financial statements, market trends, and investment analysis.\n\nUse the following context from company reports, web data, and relevant documents to answer the user query. \nAlways base your answers on the provided context and do not make unsupported claims. \n\nContext:\nTesla's revenue in 2023 increased by 15% compared to 2022. Gross margin is 25%.\n\nUser Question:\nShould I invest in Tesla for the next 5 years?\n\nInstructions:\n- Analyze the financial and business information carefully.\n- Provide long-term investment insights.\n- Give clear reasoning and avoid generic statements.\n- If the context does not provide sufficient information, say \"Insufficient data provided.\"\n- Summarize your analysis in a professional and concise manner.\n\nAnswer:\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Generation","metadata":{}},{"cell_type":"code","source":"query = \"Give me detailed fundamental analysis on Marico Bangladesh Limited\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:55:12.425008Z","iopub.execute_input":"2025-10-28T16:55:12.425315Z","iopub.status.idle":"2025-10-28T16:55:12.428638Z","shell.execute_reply.started":"2025-10-28T16:55:12.425296Z","shell.execute_reply":"2025-10-28T16:55:12.428077Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### After user query, three functions are called to get split docs from three different data source. Then, the split docs are combined and embedded with the get_combined_vector_db function. All,the embedded vectors are stored in one vector_db to retrieve relevant context for prompt augmentation.","metadata":{}},{"cell_type":"code","source":"local_pdf_split_docs = get_local_pdf_split_docs()\nweb_pdf_split_docs = get_web_pdf_split_docs(query)\nweb_search_split_docs = get_web_pdf_split_docs(query)\nclean_output_dir() \nvector_db = get_combined_vector_db(local_pdf_split_docs, web_pdf_split_docs, web_search_split_docs)\nvector_db","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"retrieved_context = rag_retrieval(vector_db, query)\nfinal_prompt = investing_prompt.format(context=retrieved_context, question=user_query)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:55:28.187720Z","iopub.status.idle":"2025-10-28T16:55:28.188102Z","shell.execute_reply.started":"2025-10-28T16:55:28.187891Z","shell.execute_reply":"2025-10-28T16:55:28.187907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ans = llm.invoke(final_prompt)\nprint(ans)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:55:28.188848Z","iopub.status.idle":"2025-10-28T16:55:28.189123Z","shell.execute_reply.started":"2025-10-28T16:55:28.188974Z","shell.execute_reply":"2025-10-28T16:55:28.188984Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Chain Construction","metadata":{}},{"cell_type":"code","source":"# from langchain.schema import Document\n# from langchain.runnables import (\n#     RunnablePassthrough,\n#     RunnableLambda,\n#     RunnableSequence,\n#     RunnableParallel,\n# )\n\n# # 2️⃣ Parallel execution of three branches\n# parallel_chain = RunnableParallel(\n#     {\n#         \"books\": RunnableSequence(\n#             [\n#                 RunnableLambda(lambda query: get_local_pdf_split_docs(query)),   # split preloaded book PDFs\n#             ]\n#         ),\n#         \"online_text\": RunnableSequence(\n#             [\n#                 RunnableLambda(lambda query: get_web_search_split_docs(query)),  # search + split text\n#             ]\n#         ),\n#         \"online_pdf\": RunnableSequence(\n#             [\n#                 RunnableLambda(lambda query: get_web_pdf_split_docs(query)),  # download + split PDFs\n#             ]\n#         ),\n#     },\n#     combine_mode=\"list\",  # returns a list of outputs for each branch\n# )\n\n# # 3️⃣ Combine embeddings and build RAG pipeline\n# combine_chain = RunnableSequence(\n#     [\n#         RunnableLambda(lambda split_docs_lists: get_combined_vector_db(*split_docs_lists)),  # combine all split docs\n#         RunnableLambda(lambda vector_db: rag_retrieval(vector_db)),                           # retrieval + augmentation\n#         RunnableLambda(lambda result: generate_answer(result)),                               # LLM generation\n#     ]\n# )\n\n# # 4️⃣ Full pipeline: sequence of parallel + combine\n# full_pipeline = RunnableSequence(\n#     [\n#         RunnablePassthrough(),\n\n#         # Step 2: Parallel execution\n#         RunnableLambda(\n#             lambda query: parallel_chain.invoke({\n#                 \"books\": query,  # load local book PDFs internally\n#                 \"online_text\": query,                # user query for web search\n#                 \"online_pdf\": query                  # user query for online PDFs\n#             })\n#         ),\n\n#         # Step 3: Combine embeddings and run RAG\n#         combine_chain\n#     ]\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:55:28.189900Z","iopub.status.idle":"2025-10-28T16:55:28.190248Z","shell.execute_reply.started":"2025-10-28T16:55:28.190082Z","shell.execute_reply":"2025-10-28T16:55:28.190096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}