{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13466316,"sourceType":"datasetVersion","datasetId":8548198},{"sourceId":426333,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":347543,"modelId":368803},{"sourceId":621762,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":467636,"modelId":483462}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q langchain faiss-cpu unstructured PyPDF2\n!pip install -q huggingface_hub\n!pip install -U langchain-community langchain-huggingface\n!pip install -q langchain-huggingface\n!pip install transformers datasets tqdm ddgs\n!pip install langchain-embedding\n!pip install InstructorEmbedding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:38:52.206675Z","iopub.execute_input":"2025-10-28T17:38:52.207354Z","iopub.status.idle":"2025-10-28T17:39:31.516892Z","shell.execute_reply.started":"2025-10-28T17:38:52.207328Z","shell.execute_reply":"2025-10-28T17:39:31.516172Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 25.1.0 which is incompatible.\ngradio 5.38.1 requires aiofiles<25.0,>=22.0, but you have aiofiles 25.1.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting langchain-community\n  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting langchain-huggingface\n  Downloading langchain_huggingface-1.0.0-py3-none-any.whl.metadata (2.1 kB)\nCollecting langchain-core<2.0.0,>=1.0.1 (from langchain-community)\n  Downloading langchain_core-1.0.1-py3-none-any.whl.metadata (3.5 kB)\nCollecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\nRequirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.5)\nRequirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.3)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\nRequirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.11.0)\nRequirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\nRequirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\nCollecting huggingface-hub<1.0.0,>=0.33.4 (from langchain-huggingface)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.19.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (25.0)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.1.10)\nCollecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.0a1)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.0)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2.4.1)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.8.3)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.37.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.37.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain-community) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\nDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading langchain_huggingface-1.0.0-py3-none-any.whl (27 kB)\nDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-1.0.1-py3-none-any.whl (467 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.1/467.1 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\nInstalling collected packages: huggingface-hub, langchain-core, langchain-text-splitters, langchain-huggingface, langchain-classic, langchain-community\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.72\n    Uninstalling langchain-core-0.3.72:\n      Successfully uninstalled langchain-core-0.3.72\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.9\n    Uninstalling langchain-text-splitters-0.3.9:\n      Successfully uninstalled langchain-text-splitters-0.3.9\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlangchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.1 which is incompatible.\nlangchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\ngradio 5.38.1 requires aiofiles<25.0,>=22.0, but you have aiofiles 25.1.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.36.0 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.0.1 langchain-huggingface-1.0.0 langchain-text-splitters-1.0.0\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.1.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nCollecting ddgs\n  Downloading ddgs-9.6.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nCollecting pyarrow>=21.0.0 (from datasets)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from ddgs) (8.3.0)\nCollecting primp>=0.15.0 (from ddgs)\n  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nCollecting lxml>=6.0.0 (from ddgs)\n  Downloading lxml-6.0.2-cp311-cp311-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\nRequirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.28.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (2025.8.3)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.10)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\nRequirement already satisfied: brotli in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.1.0)\nRequirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.3.0)\nCollecting socksio==1.* (from httpx[brotli,http2,socks]>=0.28.1->ddgs)\n  Downloading socksio-1.0.0-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (6.1.0)\nRequirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.1.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.3.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading ddgs-9.6.1-py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading socksio-1.0.0-py3-none-any.whl (12 kB)\nDownloading lxml-6.0.2-cp311-cp311-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: socksio, pyarrow, primp, lxml, ddgs\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n  Attempting uninstall: lxml\n    Found existing installation: lxml 5.4.0\n    Uninstalling lxml-5.4.0:\n      Successfully uninstalled lxml-5.4.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ddgs-9.6.1 lxml-6.0.2 primp-0.15.0 pyarrow-22.0.0 socksio-1.0.0\n\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-embedding (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-embedding\u001b[0m\u001b[31m\n\u001b[0mCollecting InstructorEmbedding\n  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl.metadata (20 kB)\nDownloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\nInstalling collected packages: InstructorEmbedding\nSuccessfully installed InstructorEmbedding-1.0.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport json\nfrom tqdm import tqdm\nfrom langchain.vectorstores import FAISS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:39:31.518588Z","iopub.execute_input":"2025-10-28T17:39:31.518869Z","iopub.status.idle":"2025-10-28T17:39:32.645428Z","shell.execute_reply.started":"2025-10-28T17:39:31.518839Z","shell.execute_reply":"2025-10-28T17:39:32.644875Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Model Load","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom langchain.llms import HuggingFacePipeline\nfrom InstructorEmbedding import INSTRUCTOR\nfrom langchain_community.embeddings import HuggingFaceInstructEmbeddings\nimport torch\n\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# print(f\"Using device: {device}\")\n\n# === 1️⃣ Load fine-tuned financial LLaMA3 model for .invoke() ===\nllm_model_path = \"/kaggle/input/investing-fine-tuned-model-llama-3-2/other/default/1\"\nllm_tokenizer = AutoTokenizer.from_pretrained(llm_model_path)\nllm_model = AutoModelForCausalLM.from_pretrained(llm_model_path, torch_dtype=torch.float16, device_map=\"cpu\")\n\nllm_pipeline = pipeline(\n    \"text-generation\",\n    model=llm_model,\n    tokenizer=llm_tokenizer,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    max_new_tokens=256,\n    temperature=0.2,\n    do_sample=False\n)\nllm = HuggingFacePipeline(pipeline=llm_pipeline)\nprint(\"✅ Fine-tuned financial model loaded.\")\n\n# === 2️⃣ Load local INSTRUCTOR embedding model ===\n# Use 4B parameter model to get better answer. \n# embedding_model_path = \"/kaggle/input/qwen-3-embedding/transformers/4b/1\"  # 4B parameters\nembedding_model_path = \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\"  # 0.6B parameters\n# embeddings_model = INSTRUCTOR(embedding_model_path, device=device)\n\nembeddings_model = HuggingFaceInstructEmbeddings(\n    model_name=embedding_model_path,\n    model_kwargs={\"device\": \"gpu\"}\n)\n\nprint(\"✅ Embedding model loaded successfully.\")\n\n# LLM\nresponse = llm.invoke(\"Get Tesla 2023 annual report\")\nprint(response)\n\n# Embeddings\nvector = embeddings_model.embed_documents(\"Tesla revenue 2023\")\nprint(len(vector))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:39:32.646061Z","iopub.execute_input":"2025-10-28T17:39:32.646340Z","iopub.status.idle":"2025-10-28T17:43:19.952509Z","shell.execute_reply.started":"2025-10-28T17:39:32.646324Z","shell.execute_reply":"2025-10-28T17:43:19.951638Z"}},"outputs":[{"name":"stderr","text":"2025-10-28 17:39:44.127332: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761673184.308051      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761673184.359778      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d7214cd222d47da8e3be84f0aaf0060"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n/tmp/ipykernel_37/802643893.py:25: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFacePipeline``.\n  llm = HuggingFacePipeline(pipeline=llm_pipeline)\n/tmp/ipykernel_37/802643893.py:33: LangChainDeprecationWarning: The class `HuggingFaceInstructEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n  embeddings_model = HuggingFaceInstructEmbeddings(\n","output_type":"stream"},{"name":"stdout","text":"✅ Fine-tuned financial model loaded.\n","output_type":"stream"},{"name":"stderr","text":"`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True}. If this is not desired, please set these values explicitly.\n","output_type":"stream"},{"name":"stdout","text":"✅ Embedding model loaded successfully.\nGet Tesla 2023 annual report\nGet Tesla 2023 annual report. (Note: The report might be available in a different language than English.) (Note: The report might be available in a different language than English.) (Note: The report might be available in a different language than English.) (Note: The report might be available in a different language than English.) (Note: The report might be available in a different language than English.) (Note: The report might be available in a different language than English.) (Note: The report might be available in a different language than English.) (Note: The report might be available in a different language than English.) (Note: The report might be available in a different language than English.) (Note: The report might be available in a different language than English.) (Note: The report might be available in a different language than English.) (Note: The report might be available in a different language than English.) (Note: The report might be available in a different language than English.) (Note: The report might be available in a different language than English.) (Note: The report might be available in a different language than English.) (Note: The report might be available in a different language than English.) (Note: The report might be\n18\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Common Functions","metadata":{}},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.documents import Document\nfrom typing import List\n\ndef get_split_docs(documents):\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=2000,\n        chunk_overlap=350\n    )\n    split_docs = text_splitter.split_documents(documents)\n    print(f\"Created {len(split_docs)} text chunks.\")\n    return split_docs\n\n\n# Example documents\ndocs = [\n    Document(page_content=\"This is the content of document 1.\"),\n    Document(page_content=\"This is the content of document 2.\")\n]\n\nchunks = get_split_docs(docs)\nprint(chunks[0].page_content)  # first chunk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:43:19.954460Z","iopub.execute_input":"2025-10-28T17:43:19.954684Z","iopub.status.idle":"2025-10-28T17:43:23.356616Z","shell.execute_reply.started":"2025-10-28T17:43:19.954667Z","shell.execute_reply":"2025-10-28T17:43:23.355723Z"}},"outputs":[{"name":"stdout","text":"Created 2 text chunks.\nThis is the content of document 1.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def get_combined_vector_db(*args):\n    combined_docs = []\n    for doc_list in args:\n        if doc_list:  # make sure it's not None or empty\n            combined_docs.extend(doc_list)\n    \n    # Create vector DB for all docs\n    combined_vector_db = FAISS.from_documents(combined_docs, embeddings_model)\n    \n    return combined_vector_db\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:43:23.357474Z","iopub.execute_input":"2025-10-28T17:43:23.358394Z","iopub.status.idle":"2025-10-28T17:43:23.362986Z","shell.execute_reply.started":"2025-10-28T17:43:23.358373Z","shell.execute_reply":"2025-10-28T17:43:23.362188Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from langchain_community.document_loaders import PyPDFLoader\ndef get_pdf_split_docs(pdf_folder):\n    pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n    all_documents = []\n    \n    for pdf_path in pdf_files:\n        loader = PyPDFLoader(pdf_path)\n        docs = loader.load()  # this returns list of Document objects\n        all_documents.extend(docs)\n        \n    split_docs = get_split_docs(all_documents)\n    \n    return split_docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:43:23.364082Z","iopub.execute_input":"2025-10-28T17:43:23.364332Z","iopub.status.idle":"2025-10-28T17:43:23.416741Z","shell.execute_reply.started":"2025-10-28T17:43:23.364310Z","shell.execute_reply":"2025-10-28T17:43:23.415918Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Three sources of information will be used. \n### 1. PDF books added to Kaggle input directory\n### 2. Web search results\n### 3. Company Annual Reports","metadata":{}},{"cell_type":"markdown","source":"# 1. PDF Embedding","metadata":{}},{"cell_type":"code","source":"pdf_folder = \"/kaggle/input/investing-books-pdf\"\nlocal_pdf_split_docs = get_pdf_split_docs(pdf_folder)\n\ndef get_local_pdf_split_docs():\n    return local_pdf_split_docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:43:23.417555Z","iopub.execute_input":"2025-10-28T17:43:23.417793Z","iopub.status.idle":"2025-10-28T17:44:44.399112Z","shell.execute_reply.started":"2025-10-28T17:43:23.417768Z","shell.execute_reply":"2025-10-28T17:44:44.398475Z"}},"outputs":[{"name":"stdout","text":"Created 8126 text chunks.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# 2. Web Search","metadata":{}},{"cell_type":"code","source":"from ddgs import DDGS\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef web_search(query, max_results=3):\n    docs = []\n\n    # Search the web using DuckDuckGo\n    with DDGS() as ddgs:\n        results = list(ddgs.text(query, max_results=max_results))\n\n    for result in results:\n        url = result.get(\"href\")\n        title = result.get(\"title\")\n\n        try:\n            # Fetch the web page content\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n\n            # Parse visible text from HTML\n            soup = BeautifulSoup(response.text, \"html.parser\")\n            for tag in soup([\"script\", \"style\", \"noscript\"]):\n                tag.extract()\n            text = \" \".join(soup.get_text(separator=\" \").split())\n\n            # Only keep if text length is reasonable\n            if len(text) > 500:\n                docs.append(Document(page_content=text, metadata={\"source\": url, \"title\": title}))\n        except Exception as e:\n            pass\n\n    # print(f\"✅ Retrieved {len(docs)} web documents from top {max_results} results.\")\n    return docs\n\n# query = \"Tesla 2025 annual revenue analysis\"\n# documents = web_search(query)\n\n# print(f\"Number of documents: {len(documents)}\")\n# print(documents[0].metadata)\n# print(documents[0].page_content[:500])  # preview text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:44:44.400237Z","iopub.execute_input":"2025-10-28T17:44:44.400873Z","iopub.status.idle":"2025-10-28T17:45:11.144373Z","shell.execute_reply.started":"2025-10-28T17:44:44.400854Z","shell.execute_reply":"2025-10-28T17:45:11.143558Z"}},"outputs":[{"name":"stdout","text":"Number of documents: 1\n{'source': 'https://assets-ir.tesla.com/tesla-contents/IR/TSLA-Q3-2025-Update.pdf', 'title': 'PDF 2025 Q3 Quarterly Update Deck - assets-ir.tesla.com'}\n%PDF-1.6 %���� 9955 0 obj < > endobj 9971 0 obj < >/Filter/FlateDecode/ID[ ]/Index[9955 24]/Info 9954 0 R/Length 82/Prev 5982309/Root 9956 0 R/Size 9979/Type/XRef/W[1 2 1]>>stream h�bbd\u0010``b`�\u0006\u0012��AD\u0016�`�\u000e\"XA�\u0004\u00101\u000f$q �n �� $\u0018���B\u0019��?\u0010�\u000e$����c\u0001\u0003\u0013�Z\u0011H \u0003#V��m��\u0000\u0001\u0006\u0000�? $ endstream endobj startxref 0 %%EOF 9978 0 obj < >stream h�b```\u0002�; \\ ,� \" ��������� �\u0002 H00 \u0000)w\u0010�`\u0004\u0012l?\u000e��29pg\u00160�3j<\u0010�Ͱ}\u0003��vpҿ\u0000\u0015�\u0002�� ���u�\u000f��Gj殼��l=���ғ� C\u0015\u000f0(0z�1�m\u0010�e:7��\u0005C�d��{�W��\u000e$�Wo\u0005�>\"��b�*Qĩ����j���\u0004��ˑ�Y,&!�l����\\\u0007 &L��MY�bz\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def get_web_search_split_docs(query, max_results=3):\n    docs = web_search(query, max_results)\n    split_docs = get_split_docs(docs)\n    return split_docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:45:11.145308Z","iopub.execute_input":"2025-10-28T17:45:11.145543Z","iopub.status.idle":"2025-10-28T17:45:11.149294Z","shell.execute_reply.started":"2025-10-28T17:45:11.145526Z","shell.execute_reply":"2025-10-28T17:45:11.148570Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# 3. Web PDF Retriever","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nimport json\n\ndef analyze_query_with_llm(query: str):\n    \"\"\"\n    Use the fine-tuned LLM to decide if annual report retrieval is required,\n    and extract the company name and year if applicable.\n\n    Returns a dictionary like:\n    {\n        \"required\": True,\n        \"company\": \"Tesla\",\n        \"year\": \"2024\"\n    }\n    \"\"\"\n    prompt = f\"\"\"\n    You are an intelligent financial assistant. \n    Your task is to analyze the user query and decide:\n    1. Whether it requires downloading a company's annual report.\n    2. If yes, extract the company name and the year of the report.\n    \n    Respond strictly in this JSON format:\n    {{\n      \"required\": true or false,\n      \"company\": \"Company name or null\",\n      \"year\": \"Year or null\"\n    }}\n    \n    User query: \"{query}\"\n    \"\"\"\n    # Tokenize input\n    inputs = llm_tokenizer(prompt, return_tensors=\"pt\").to(llm_model.device)\n\n    # Generate model output\n    outputs = llm_model.generate(\n        **inputs,\n        max_new_tokens=256,\n        temperature=0.2,\n        do_sample=False\n    )\n\n    response_text = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Try to extract JSON response from model output\n    try:\n        json_start = response_text.find(\"{\")\n        json_end = response_text.rfind(\"}\") + 1\n        json_str = response_text[json_start:json_end]\n        result = json.loads(json_str)\n    except Exception as e:\n        print(\"⚠️ Could not parse model output properly. Raw output:\")\n        # print(response_text)\n        result = {\"required\": False, \"company\": None, \"year\": None}\n\n    return result\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:45:11.151670Z","iopub.execute_input":"2025-10-28T17:45:11.151909Z","iopub.status.idle":"2025-10-28T17:45:11.163160Z","shell.execute_reply.started":"2025-10-28T17:45:11.151894Z","shell.execute_reply":"2025-10-28T17:45:11.162548Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from ddgs import DDGS\n\ndef search_pdfs(company_name, year=2024, max_results=5):\n    query = f\"{company_name} {year} annual report filetype:pdf\"\n    with DDGS() as ddgs:\n        results = list(ddgs.text(query, max_results=max_results))\n    pdf_links = [r['href'] for r in results if r['href'].endswith('.pdf')]\n    return pdf_links\n\n# Example:\npdf_urls = search_pdfs(\"Square Pharma\", 2024)\nprint(pdf_urls)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:45:11.163838Z","iopub.execute_input":"2025-10-28T17:45:11.164052Z","iopub.status.idle":"2025-10-28T17:45:11.575649Z","shell.execute_reply.started":"2025-10-28T17:45:11.164038Z","shell.execute_reply":"2025-10-28T17:45:11.574858Z"}},"outputs":[{"name":"stdout","text":"[]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import requests\n\ndef download_pdf(pdf_links, save_dir):\n    os.makedirs(save_dir, exist_ok=True)  # ensure directory exists\n\n    for i, url in enumerate(pdf_links, 1):\n        response = requests.get(url, stream=True)\n        if response.status_code == 200 and 'application/pdf' in response.headers.get('Content-Type', ''):\n            file_path = os.path.join(save_dir, f\"web_pdf_{i}.pdf\")\n            with open(file_path, 'wb') as f:\n                for chunk in response.iter_content(1024):\n                    f.write(chunk)\n            print(f\"✅ Downloaded: {file_path}\")\n        else:\n            print(f\"❌ Failed to download from {url}\")\n\n# for i in range(len(pdf_urls)):\n# download_pdf(pdf_urls[4], \"apple_2024_annual_report.pdf\")\n# download_pdf(\"https://www.squarepharma.com.bd/downloads/Square%20Pharma_AR_24%20dt-24-11-24_compressed_1.pdf\", \"hjd.pdf\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:45:11.576494Z","iopub.execute_input":"2025-10-28T17:45:11.576732Z","iopub.status.idle":"2025-10-28T17:45:11.582290Z","shell.execute_reply.started":"2025-10-28T17:45:11.576706Z","shell.execute_reply":"2025-10-28T17:45:11.581495Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import shutil\ndef clean_output_dir():\n    path = \"/kaggle/working/\"\n    for f in os.listdir(path):\n        fp = os.path.join(path, f)\n        if os.path.isfile(fp) or os.path.islink(fp):\n            os.unlink(fp)\n        elif os.path.isdir(fp):\n            shutil.rmtree(fp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:45:11.583104Z","iopub.execute_input":"2025-10-28T17:45:11.583370Z","iopub.status.idle":"2025-10-28T17:45:11.595771Z","shell.execute_reply.started":"2025-10-28T17:45:11.583348Z","shell.execute_reply":"2025-10-28T17:45:11.595050Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def get_web_pdf_split_docs(query, max_results=5):\n    result = analyze_query_with_llm(query) \n    \n    pdf_links = search_pdfs(result['company'], result['year'])\n    \n    output_dir = \"/kaggle/working/\"\n    \n    download_pdf(pdf_links, output_dir)\n    \n    split_docs = get_pdf_split_docs(output_dir)\n\n    return split_docs\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:45:11.596414Z","iopub.execute_input":"2025-10-28T17:45:11.596571Z","iopub.status.idle":"2025-10-28T17:45:11.609025Z","shell.execute_reply.started":"2025-10-28T17:45:11.596559Z","shell.execute_reply":"2025-10-28T17:45:11.608302Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Retrieval","metadata":{}},{"cell_type":"code","source":"def rag_retrieval(vector_db, query):\n    \"\"\"\n    Perform similarity search on the vector DB using the query.\n    Returns top relevant documents.\n    \"\"\"\n    return vector_db.similarity_search(query, k=5)  # top 5 results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:45:11.609794Z","iopub.execute_input":"2025-10-28T17:45:11.610034Z","iopub.status.idle":"2025-10-28T17:45:11.621797Z","shell.execute_reply.started":"2025-10-28T17:45:11.610007Z","shell.execute_reply":"2025-10-28T17:45:11.621040Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Augmentation","metadata":{}},{"cell_type":"code","source":"from langchain_core.prompts import PromptTemplate # <-- Fix this line\n\n# Define the template string\nprompt = \"\"\"\nYou are a financial and investing expert specializing in long-term investing, company analysis, and stock market strategies. \nYou have deep knowledge of business fundamentals, financial statements, market trends, and investment analysis.\n\nUse the following context from company reports, web data, and relevant documents to answer the user query. \nAlways base your answers on the provided context and do not make unsupported claims. \n\nContext:\n{context}\n\nUser Question:\n{question}\n\nInstructions:\n- Analyze the financial and business information carefully.\n- Provide long-term investment insights.\n- Give clear reasoning and avoid generic statements.\n- If the context does not provide sufficient information, say \"Insufficient data provided.\"\n- Summarize your analysis in a professional and concise manner.\n\nAnswer:\n\"\"\"\n\n# Create a PromptTemplate instance\ninvesting_prompt = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=prompt\n)\n\n# Example usage\nretrieved_context = \"Tesla's revenue in 2023 increased by 15% compared to 2022. Gross margin is 25%.\"\nuser_query = \"Should I invest in Tesla for the next 5 years?\"\n\nfinal_prompt = investing_prompt.format(context=retrieved_context, question=user_query)\nprint(final_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:45:11.622580Z","iopub.execute_input":"2025-10-28T17:45:11.623147Z","iopub.status.idle":"2025-10-28T17:45:11.654198Z","shell.execute_reply.started":"2025-10-28T17:45:11.623096Z","shell.execute_reply":"2025-10-28T17:45:11.653645Z"}},"outputs":[{"name":"stdout","text":"\nYou are a financial and investing expert specializing in long-term investing, company analysis, and stock market strategies. \nYou have deep knowledge of business fundamentals, financial statements, market trends, and investment analysis.\n\nUse the following context from company reports, web data, and relevant documents to answer the user query. \nAlways base your answers on the provided context and do not make unsupported claims. \n\nContext:\nTesla's revenue in 2023 increased by 15% compared to 2022. Gross margin is 25%.\n\nUser Question:\nShould I invest in Tesla for the next 5 years?\n\nInstructions:\n- Analyze the financial and business information carefully.\n- Provide long-term investment insights.\n- Give clear reasoning and avoid generic statements.\n- If the context does not provide sufficient information, say \"Insufficient data provided.\"\n- Summarize your analysis in a professional and concise manner.\n\nAnswer:\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Generation","metadata":{}},{"cell_type":"markdown","source":"### After user query, three functions are called to get split docs from three different data source. Then, the split docs are combined and embedded with the get_combined_vector_db function. All,the embedded vectors are stored in one vector_db to retrieve relevant context for prompt augmentation.","metadata":{}},{"cell_type":"code","source":"def get_vector_db(query):\n    clean_output_dir() \n    \n    local_pdf_split_docs = get_local_pdf_split_docs()    \n    web_pdf_split_docs = get_web_pdf_split_docs(query)\n    web_search_split_docs = get_web_pdf_split_docs(query)\n    \n    vector_db = get_combined_vector_db(local_pdf_split_docs, web_pdf_split_docs, web_search_split_docs)\n    \n    return vector_db","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:45:11.654834Z","iopub.execute_input":"2025-10-28T17:45:11.655058Z","iopub.status.idle":"2025-10-28T17:45:11.659262Z","shell.execute_reply.started":"2025-10-28T17:45:11.655039Z","shell.execute_reply":"2025-10-28T17:45:11.658478Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def generate_ans(query):\n    vector_db = get_vector_db(query)    # Get Vector Database\n    \n    retrieved_context = rag_retrieval(vector_db, query)    # Retrieve\n    \n    final_prompt = investing_prompt.format(context=retrieved_context, question=user_query)   # Augmentation\n    \n    ans = llm.invoke(final_prompt)    # Generation\n    \n    print(ans)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:45:11.660066Z","iopub.execute_input":"2025-10-28T17:45:11.660536Z","iopub.status.idle":"2025-10-28T17:45:11.671592Z","shell.execute_reply.started":"2025-10-28T17:45:11.660515Z","shell.execute_reply":"2025-10-28T17:45:11.670955Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"query = \"Give me detailed fundamental analysis on Marico Bangladesh Limited\"\ngenerate_ans(query)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:45:11.672256Z","iopub.execute_input":"2025-10-28T17:45:11.673096Z","iopub.status.idle":"2025-10-28T17:53:01.741786Z","shell.execute_reply.started":"2025-10-28T17:45:11.673074Z","shell.execute_reply":"2025-10-28T17:53:01.740499Z"}},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⚠️ Could not parse model output properly. Raw output:\n✅ Downloaded: /kaggle/working/web_pdf_1.pdf\n✅ Downloaded: /kaggle/working/web_pdf_2.pdf\n✅ Downloaded: /kaggle/working/web_pdf_3.pdf\n✅ Downloaded: /kaggle/working/web_pdf_4.pdf\n❌ Failed to download from https://www.sec.gov/Archives/edgar/data/1326380/000119312525093732/d792750dars.pdf\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Created 1248 text chunks.\n⚠️ Could not parse model output properly. Raw output:\n❌ Failed to download from https://dam.assets.ohio.gov/image/upload/com.ohio.gov/UNFD/Report+Funds/UNCF_Negative_Report_Guide.pdf\n✅ Downloaded: /kaggle/working/web_pdf_2.pdf\n❌ Failed to download from https://www.ftc.gov/system/files/ftc_gov/pdf/2025-acquired-person-notification-report-form-for-certain-mergers-acquisitions-01-27-25.pdf\n❌ Failed to download from https://dat.maryland.gov/SDAT+Forms/PPR_Forms/2024Form1_Instructions.pdf\n✅ Downloaded: /kaggle/working/web_pdf_5.pdf\nCreated 2782 text chunks.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/938959626.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Give me detailed fundamental analysis on Marico Bangladesh Limited\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerate_ans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_37/4290266586.py\u001b[0m in \u001b[0;36mgenerate_ans\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_ans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mvector_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vector_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Get Vector Database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mretrieved_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrag_retrieval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_db\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Retrieve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/2663597579.py\u001b[0m in \u001b[0;36mget_vector_db\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mweb_search_split_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_web_pdf_split_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclean_output_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mvector_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_combined_vector_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_pdf_split_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweb_pdf_split_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweb_search_split_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvector_db\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/3125876191.py\u001b[0m in \u001b[0;36mget_combined_vector_db\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Create vector DB for all docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcombined_vector_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcombined_vector_db\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/vectorstores/base.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[0mfaiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \"\"\"\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m         return cls.__from(\n\u001b[1;32m   1045\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/embeddings/huggingface.py\u001b[0m in \u001b[0;36membed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \"\"\"\n\u001b[1;32m    226\u001b[0m         \u001b[0minstruction_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_instruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         embeddings = self.client.encode(\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0minstruction_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_progress\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/InstructorEmbedding/instructor.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m                 \u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0moutput_value\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'token_embeddings'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m         }\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrans_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_embeddings\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen3/modeling_qwen3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    459\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen3/modeling_qwen3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen3/modeling_qwen3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mattention_interface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALL_ATTENTION_FUNCTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         attn_output, attn_weights = attention_interface(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_attention.py\u001b[0m in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mis_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 420.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 147.12 MiB is free. Process 3317 has 15.74 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 420.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 147.12 MiB is free. Process 3317 has 15.74 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":20},{"cell_type":"markdown","source":" CUDA out of memory. Tried to allocate 420.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 147.12 MiB is free. Process 3317 has 15.74 GiB memory in use.","metadata":{}},{"cell_type":"markdown","source":"# Chain Construction","metadata":{}},{"cell_type":"code","source":"# from langchain.schema import Document\n# from langchain.runnables import (\n#     RunnablePassthrough,\n#     RunnableLambda,\n#     RunnableSequence,\n#     RunnableParallel,\n# )\n\n# # 2️⃣ Parallel execution of three branches\n# parallel_chain = RunnableParallel(\n#     {\n#         \"books\": RunnableSequence(\n#             [\n#                 RunnableLambda(lambda query: get_local_pdf_split_docs(query)),   # split preloaded book PDFs\n#             ]\n#         ),\n#         \"online_text\": RunnableSequence(\n#             [\n#                 RunnableLambda(lambda query: get_web_search_split_docs(query)),  # search + split text\n#             ]\n#         ),\n#         \"online_pdf\": RunnableSequence(\n#             [\n#                 RunnableLambda(lambda query: get_web_pdf_split_docs(query)),  # download + split PDFs\n#             ]\n#         ),\n#     },\n#     combine_mode=\"list\",  # returns a list of outputs for each branch\n# )\n\n# # 3️⃣ Combine embeddings and build RAG pipeline\n# combine_chain = RunnableSequence(\n#     [\n#         RunnableLambda(lambda split_docs_lists: get_combined_vector_db(*split_docs_lists)),  # combine all split docs\n#         RunnableLambda(lambda vector_db: rag_retrieval(vector_db)),                           # retrieval + augmentation\n#         RunnableLambda(lambda result: generate_answer(result)),                               # LLM generation\n#     ]\n# )\n\n# # 4️⃣ Full pipeline: sequence of parallel + combine\n# full_pipeline = RunnableSequence(\n#     [\n#         RunnablePassthrough(),\n\n#         # Step 2: Parallel execution\n#         RunnableLambda(\n#             lambda query: parallel_chain.invoke({\n#                 \"books\": query,  # load local book PDFs internally\n#                 \"online_text\": query,                # user query for web search\n#                 \"online_pdf\": query                  # user query for online PDFs\n#             })\n#         ),\n\n#         # Step 3: Combine embeddings and run RAG\n#         combine_chain\n#     ]\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:53:01.742367Z","iopub.status.idle":"2025-10-28T17:53:01.742595Z","shell.execute_reply.started":"2025-10-28T17:53:01.742486Z","shell.execute_reply":"2025-10-28T17:53:01.742496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}