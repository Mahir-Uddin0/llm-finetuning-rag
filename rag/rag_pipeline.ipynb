{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13466316,"sourceType":"datasetVersion","datasetId":8548198},{"sourceId":621070,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":467088,"modelId":482912}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install serpapi beautifulsoup4 requests newspaper3k","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T16:02:49.411281Z","iopub.execute_input":"2025-10-27T16:02:49.411536Z","iopub.status.idle":"2025-10-27T16:02:49.416477Z","shell.execute_reply.started":"2025-10-27T16:02:49.411508Z","shell.execute_reply":"2025-10-27T16:02:49.415525Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport re\n\n# -----------------------------\n# CONFIGURATION\n# -----------------------------\nSERPAPI_KEY = \"79d5a10900dc6aae037bdbb5a52937fb569ad2e4\"  # replace with your key\nSEARCH_ENGINE = \"google\"\n\n# -----------------------------\n# 1. Web Search Function (SerpAPI)\n# -----------------------------\ndef web_search(query, num_results=5):\n    \"\"\"Search web using SerpAPI and return top result URLs.\"\"\"\n    url = \"https://serpapi.com/search\"\n    params = {\n        \"engine\": SEARCH_ENGINE,\n        \"q\": query,\n        \"api_key\": SERPAPI_KEY,\n        \"num\": num_results,\n        \"hl\": \"en\",\n    }\n    res = requests.get(url, params=params)\n    data = res.json()\n\n    results = []\n    for item in data.get(\"organic_results\", []):\n        results.append({\n            \"title\": item.get(\"title\"),\n            \"link\": item.get(\"link\"),\n            \"snippet\": item.get(\"snippet\")\n        })\n    return results\n\n\n# -----------------------------\n# 2. Clean Text Extraction from URL\n# -----------------------------\ndef extract_text_from_url(url, min_length=200):\n    \"\"\"Extracts main content text from a web page using BeautifulSoup.\"\"\"\n    try:\n        html = requests.get(url, timeout=10).text\n        soup = BeautifulSoup(html, \"html.parser\")\n\n        # Remove scripts, styles, headers, footers, navs\n        for tag in soup([\"script\", \"style\", \"noscript\", \"header\", \"footer\", \"form\", \"nav\", \"aside\"]):\n            tag.extract()\n\n        text = soup.get_text(separator=\" \")\n        # clean multiple spaces\n        text = re.sub(r\"\\s+\", \" \", text).strip()\n\n        if len(text) >= min_length:\n            return text\n        else:\n            return \"\"\n\n    except Exception as e:\n        print(f\"⚠️ Error parsing {url}: {e}\")\n        return \"\"\n\n\n# -----------------------------\n# 3. Complete Web Retriever\n# -----------------------------\ndef web_retriever(query, num_results=3):\n    \"\"\"Search the web and return cleaned content of top results.\"\"\"\n    search_results = web_search(query, num_results)\n    pages = []\n\n    for result in search_results:\n        link = result[\"link\"]\n        text = extract_text_from_url(link)\n        if text:\n            pages.append({\n                \"title\": result[\"title\"],\n                \"url\": link,\n                \"snippet\": result.get(\"snippet\", \"\"),\n                \"content\": text[:5000]  # trim to avoid huge texts\n            })\n    return pages\n\n\n# -----------------------------\n# 4. Example Usage\n# -----------------------------\n# if __name__ == \"__main__\":\nquery = \"Apple 2024 annual report summary\"\nresults = web_retriever(query, num_results=3)\n\nfor i, r in enumerate(results):\n    print(f\"--- Result {i+1} ---\")\n    print(f\"Title: {r['title']}\")\n    print(f\"URL: {r['url']}\")\n    print(f\"Snippet: {r['snippet']}\")\n    print(f\"Content preview:\\n{r['content'][:500]}...\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T13:53:34.299489Z","iopub.execute_input":"2025-10-27T13:53:34.299856Z","iopub.status.idle":"2025-10-27T13:53:34.560622Z","shell.execute_reply.started":"2025-10-27T13:53:34.299828Z","shell.execute_reply":"2025-10-27T13:53:34.559764Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport re\n\nSERPAPI_KEY = \"79d5a10900dc6aae037bdbb5a52937fb569ad2e4\"  # replace with your key\nSEARCH_ENGINE = \"google\"\n\ndef web_search(query, num_results=5):\n    \"\"\"Search web using SerpAPI and return top result URLs.\"\"\"\n    url = \"https://serpapi.com/search\"\n    params = {\n        \"engine\": SEARCH_ENGINE,\n        \"q\": query,\n        \"api_key\": SERPAPI_KEY,\n        \"num\": num_results,\n        \"hl\": \"en\",\n    }\n\n    res = requests.get(url, params=params)\n    try:\n        data = res.json()\n    except Exception as e:\n        print(\"⚠️ Could not parse JSON:\", e)\n        print(res.text)\n        return []\n\n    # Debug: print keys to see what SerpAPI returned\n    print(\"Keys in JSON response:\", data.keys())\n\n    results = []\n    if \"organic_results\" not in data:\n        print(\"⚠️ No organic_results found in response\")\n        return []\n\n    for item in data[\"organic_results\"]:\n        results.append({\n            \"title\": item.get(\"title\"),\n            \"link\": item.get(\"link\"),\n            \"snippet\": item.get(\"snippet\")\n        })\n    return results\n\ndef extract_text_from_url(url, min_length=200):\n    try:\n        html = requests.get(url, timeout=10).text\n        soup = BeautifulSoup(html, \"html.parser\")\n        for tag in soup([\"script\", \"style\", \"noscript\", \"header\", \"footer\", \"form\", \"nav\", \"aside\"]):\n            tag.extract()\n        text = soup.get_text(separator=\" \")\n        text = re.sub(r\"\\s+\", \" \", text).strip()\n        if len(text) >= min_length:\n            return text\n        else:\n            return \"\"\n    except Exception as e:\n        print(f\"⚠️ Error parsing {url}: {e}\")\n        return \"\"\n\ndef web_retriever(query, num_results=3):\n    search_results = web_search(query, num_results)\n    if not search_results:\n        print(\"⚠️ No search results returned\")\n        return []\n\n    pages = []\n    for result in search_results:\n        link = result[\"link\"]\n        text = extract_text_from_url(link)\n        if text:\n            pages.append({\n                \"title\": result[\"title\"],\n                \"url\": link,\n                \"snippet\": result.get(\"snippet\", \"\"),\n                \"content\": text[:5000]\n            })\n    return pages\n\nif __name__ == \"__main__\":\n    query = \"Apple 2024 annual report summary\"\n    results = web_retriever(query, num_results=3)\n\n    if not results:\n        print(\"⚠️ No pages retrieved\")\n    else:\n        for i, r in enumerate(results):\n            print(f\"--- Result {i+1} ---\")\n            print(f\"Title: {r['title']}\")\n            print(f\"URL: {r['url']}\")\n            print(f\"Snippet: {r['snippet']}\")\n            print(f\"Content preview:\\n{r['content'][:500]}...\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T13:57:30.452882Z","iopub.execute_input":"2025-10-27T13:57:30.453188Z","iopub.status.idle":"2025-10-27T13:57:30.714710Z","shell.execute_reply.started":"2025-10-27T13:57:30.453166Z","shell.execute_reply":"2025-10-27T13:57:30.713830Z"}},"outputs":[{"name":"stdout","text":"Keys in JSON response: dict_keys(['error'])\n⚠️ No organic_results found in response\n⚠️ No search results returned\n⚠️ No pages retrieved\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport re\nimport urllib.parse\n\n# -----------------------------\n# 1. Get top Google search URLs\n# -----------------------------\ndef google_search(query, num_results=5):\n    \"\"\"Scrape top Google search results (titles + links).\"\"\"\n    query = urllib.parse.quote_plus(query)\n    url = f\"https://www.google.com/search?q={query}&num={num_results}\"\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n                      \"(KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    res = requests.get(url, headers=headers)\n    soup = BeautifulSoup(res.text, \"html.parser\")\n\n    results = []\n    for g in soup.find_all('div', class_='tF2Cxc'):\n        a_tag = g.find('a')\n        title_tag = g.find('h3')\n        if a_tag and title_tag:\n            link = a_tag['href']\n            title = title_tag.get_text()\n            snippet_tag = g.find('span', class_='aCOpRe')\n            snippet = snippet_tag.get_text() if snippet_tag else \"\"\n            results.append({\n                \"title\": title,\n                \"link\": link,\n                \"snippet\": snippet\n            })\n    return results[:num_results]\n\n# -----------------------------\n# 2. Extract text from web page\n# -----------------------------\ndef extract_text_from_url(url, min_length=200):\n    \"\"\"Extract main content text from a web page.\"\"\"\n    try:\n        html = requests.get(url, timeout=10).text\n        soup = BeautifulSoup(html, \"html.parser\")\n        for tag in soup([\"script\", \"style\", \"noscript\", \"header\", \"footer\", \"form\", \"nav\", \"aside\"]):\n            tag.extract()\n        text = soup.get_text(separator=\" \")\n        text = re.sub(r\"\\s+\", \" \", text).strip()\n        if len(text) >= min_length:\n            return text\n        else:\n            return \"\"\n    except Exception as e:\n        print(f\"⚠️ Error parsing {url}: {e}\")\n        return \"\"\n\n# -----------------------------\n# 3. Full Web Retriever\n# -----------------------------\ndef web_retriever(query, num_results=3):\n    search_results = google_search(query, num_results)\n    if not search_results:\n        print(\"⚠️ No search results returned\")\n        return []\n\n    pages = []\n    for result in search_results:\n        link = result[\"link\"]\n        text = extract_text_from_url(link)\n        if text:\n            pages.append({\n                \"title\": result[\"title\"],\n                \"url\": link,\n                \"snippet\": result.get(\"snippet\", \"\"),\n                \"content\": text[:5000]  # trim\n            })\n    return pages\n\n# -----------------------------\n# 4. Example Usage\n# -----------------------------\nif __name__ == \"__main__\":\n    query = \"Apple 2024 annual report summary\"\n    results = web_retriever(query, num_results=3)\n\n    if not results:\n        print(\"⚠️ No pages retrieved\")\n    else:\n        for i, r in enumerate(results):\n            print(f\"--- Result {i+1} ---\")\n            print(f\"Title: {r['title']}\")\n            print(f\"URL: {r['url']}\")\n            print(f\"Snippet: {r['snippet']}\")\n            print(f\"Content preview:\\n{r['content'][:500]}...\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T13:59:05.902874Z","iopub.execute_input":"2025-10-27T13:59:05.903191Z","iopub.status.idle":"2025-10-27T13:59:06.047201Z","shell.execute_reply.started":"2025-10-27T13:59:05.903168Z","shell.execute_reply":"2025-10-27T13:59:06.046265Z"}},"outputs":[{"name":"stdout","text":"⚠️ No search results returned\n⚠️ No pages retrieved\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}